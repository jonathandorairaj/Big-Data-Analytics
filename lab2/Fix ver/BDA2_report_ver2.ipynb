{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45621637",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97be7bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Q1. What are the lowest and highest temperatures measured each year for the period 1950-2014?\n",
    "Provide the lists sorted in the descending order with respect to the maximum temperature. \n",
    "In this exercise you will use the temperature-readings.csv file.\n",
    "The output should at least contain the following information (You can also include a Station column so that you may find multiple stations that record the highest (lowest)\n",
    "temperature.)\n",
    "\n",
    "year, station with the max, maxValue ORDER BY maxValue DESC  \n",
    "year, station with the min, minValue ORDER BY minValue DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcefa41d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "sc = SparkContext(appName = 'exercise 1')\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#sqlContext = HiveContext(sc)\n",
    "\n",
    "# This path is to the file on hdfs\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "lines = temperature_file.map(lambda line: line.split(\";\"))\n",
    "\n",
    "# (key, value) = (year,temperature)\n",
    "tempReadingsRow = lines.map(lambda p: (p[0],int(p[1].split(\"-\")[0]),float(p[3]) ))\n",
    "\n",
    "## Inferring schema and registering the Dataframe as a table\n",
    "tempReadingsString = [\"station\",\"year\",\"value\"]\n",
    "\n",
    "schemaTempReadings = sqlContext.createDataFrame(tempReadingsRow,tempReadingsString)\n",
    "\n",
    "# Register the DataFrame as a table\n",
    "schemaTempReadings.registerTempTable(\"tempReadingsTable\")\n",
    "\n",
    "#filter years 1950-2014 \n",
    "schemaTempReadings = schemaTempReadings.filter((schemaTempReadings[\"year\"]>= 1950) & (schemaTempReadings[\"year\"]<= 2014))\n",
    "#schemaTempReadings = schemaTempReadings.collect()\n",
    "\n",
    "\n",
    "schemaTempReadings =  schemaTempReadings.select(['year','station','value'])\n",
    "\n",
    "schemaTempReadingsMin = schemaTempReadings.groupBy('year','station').agg(F.min('value').alias('min'))\n",
    "schemaTempReadingsMinYear = schemaTempReadingsMin.groupBy('year').agg(F.min('min').alias('min'))\n",
    "\n",
    "year_station_mintemp = schemaTempReadingsMin.join(schemaTempReadingsMinYear, ['year', 'min']).select('year', 'station', 'min').orderBy(['min'],ascending = False)\n",
    "\n",
    "\n",
    "schemaTempReadingsMax = schemaTempReadings.groupBy('year','station').agg(F.max('value').alias('max'))\n",
    "\n",
    "schemaTempReadingsMaxYear = schemaTempReadingsMax.groupBy('year').agg(F.max('max').alias('max'))\n",
    "\n",
    "year_station_maxtemp = schemaTempReadingsMax.join(schemaTempReadingsMaxYear, ['year', 'max']).select('year', 'station', 'max').orderBy(['max'],ascending = False)\n",
    "\n",
    "year_station_mintemp.rdd.saveAsTextFile(\"BDA/output/q1_min\")\n",
    "year_station_maxtemp.rdd.saveAsTextFile(\"BDA/output/q1_max\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2dca68",
   "metadata": {},
   "source": [
    "* output \n",
    "\n",
    "#### year, station with the max, maxValue ORDER BY maxValue DESC  \n",
    "\n",
    "Row(year=1975, station=u'86200', max=36.1)  \n",
    "Row(year=1992, station=u'63600', max=35.4)  \n",
    "Row(year=1994, station=u'117160', max=34.7)  \n",
    "Row(year=2010, station=u'75250', max=34.4)  \n",
    "Row(year=2014, station=u'96560', max=34.4)  \n",
    "Row(year=1989, station=u'63050', max=33.9)  \n",
    "Row(year=1982, station=u'94050', max=33.8)  \n",
    "Row(year=1968, station=u'137100', max=33.7)  \n",
    "Row(year=1966, station=u'151640', max=33.5)  \n",
    "Row(year=2002, station=u'78290', max=33.3)    \n",
    "Row(year=1983, station=u'98210', max=33.3)   \n",
    "Row(year=1970, station=u'103080', max=33.2)  \n",
    "Row(year=1986, station=u'76470', max=33.2)  \n",
    "Row(year=1956, station=u'145340', max=33.0)  \n",
    "Row(year=2000, station=u'62400', max=33.0)  \n",
    "Row(year=1959, station=u'65160', max=32.8)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b213c78e",
   "metadata": {},
   "source": [
    "#### year, station with the min, minValue ORDER BY minValue DESC\n",
    "\n",
    "Row(year=1990, station=u'147270', min=-35.0)  \n",
    "Row(year=1990, station=u'166870', min=-35.0)  \n",
    "Row(year=1952, station=u'192830', min=-35.5)  \n",
    "Row(year=1974, station=u'166870', min=-35.6)  \n",
    "Row(year=1974, station=u'179950', min=-35.6)  \n",
    "Row(year=1954, station=u'113410', min=-36.0)  \n",
    "Row(year=1992, station=u'179960', min=-36.1)  \n",
    "Row(year=1975, station=u'157860', min=-37.0)   \n",
    "Row(year=1972, station=u'167860', min=-37.5)  \n",
    "Row(year=2000, station=u'169860', min=-37.6)  \n",
    "Row(year=1995, station=u'182910', min=-37.6)  \n",
    "Row(year=1957, station=u'159970', min=-37.8)  \n",
    "Row(year=1983, station=u'191900', min=-38.2)  \n",
    "Row(year=1989, station=u'166870', min=-38.2)  \n",
    "Row(year=1953, station=u'183760', min=-38.4)  \n",
    "Row(year=2009, station=u'179960', min=-38.5)  \n",
    "Row(year=1993, station=u'191900', min=-39.0)  \n",
    "Row(year=1984, station=u'191900', min=-39.2)  \n",
    "Row(year=1984, station=u'123480', min=-39.2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dd129d",
   "metadata": {},
   "source": [
    "### Q2 Count the number of readings for each month in the period of 1950-2014 which are higher than 10 degrees.\n",
    "Repeat the exercise,this time taking only distinct readings from each station.  \n",
    "That is, if a station reported a reading above 10 degrees in some month, then itappears only once in the count for that month.  \n",
    "In this exercise you will use the temperature-readings.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e28bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sc = SparkContext(appName=\"exercise 1\")\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "lines = temperature_file.map(lambda line: line.split(\";\"))\n",
    "\n",
    "year_month_temperature = lines.map(lambda x: (x[1][0:4],x[1][5:7],x[0],float(x[3])))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"month\", StringType(), True),\n",
    "    StructField(\"station\", StringType(), True),\n",
    "    StructField(\"value\", FloatType(), True)\n",
    "])\n",
    "\n",
    "year_month_temperature_df = sqlContext.createDataFrame(year_month_temperature, schema)\n",
    "\n",
    "filtered_year_month_temperature = year_month_temperature_df.filter((year_month_temperature_df[\"year\"] >= \"1950\") & (year_month_temperature_df[\"year\"] <= \"2014\")& (year_month_temperature_df[\"value\"] > 10))\n",
    "\n",
    "count_temp = filtered_year_month_temperature.groupBy([\"year\", \"month\"]).count()\n",
    "sort_count = count_temp.sort(\"count\", ascending = False)\n",
    "sort_count_combine = sort_count.rdd.coalesce(1)\n",
    "sort_count_combine = sort_count_combine.sortBy(lambda x: x[2], ascending=False)\n",
    "sort_count_combine.saveAsTextFile(\"BDA/output/l2_not_distinct\")\n",
    "\n",
    "distinct_temp = filtered_year_month_temperature.select([\"year\", \"month\", \"station\"]).distinct()\n",
    "count_dist_temp = distinct_temp.groupBy([\"year\", \"month\"]).count()\n",
    "sort_count_dist = count_dist_temp.sort(\"count\", ascending = False)\n",
    "sort_count_dist = sort_count_dist.rdd.coalesce(1)\n",
    "sort_count_dist = sort_count_dist.sortBy(lambda x: x[2], ascending=False)\n",
    "sort_count_dist.saveAsTextFile(\"BDA/output/l2_distinct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c3fc05",
   "metadata": {},
   "source": [
    "### output of l2_not_distinct  \n",
    "#### Year, month, count\n",
    "\n",
    "Row(year=u'2014', month=u'07', count=147681)  \n",
    "Row(year=u'2011', month=u'07', count=146656)  \n",
    "Row(year=u'2010', month=u'07', count=143419)  \n",
    "Row(year=u'2012', month=u'07', count=137477)  \n",
    "Row(year=u'2013', month=u'07', count=133657)  \n",
    "Row(year=u'2009', month=u'07', count=133008)  \n",
    "Row(year=u'2011', month=u'08', count=132734)  \n",
    "Row(year=u'2009', month=u'08', count=128349)  \n",
    "Row(year=u'2013', month=u'08', count=128235)  \n",
    "Row(year=u'2003', month=u'07', count=128133)  \n",
    "  \n",
    "### output of l2_distinct \n",
    "#### Year, month, count\n",
    "\n",
    "Row(year=u'1972', month=u'10', count=378)  \n",
    "Row(year=u'1973', month=u'05', count=377)  \n",
    "Row(year=u'1973', month=u'06', count=377)  \n",
    "Row(year=u'1972', month=u'08', count=376)  \n",
    "Row(year=u'1973', month=u'09', count=376)  \n",
    "Row(year=u'1972', month=u'06', count=375)  \n",
    "Row(year=u'1972', month=u'09', count=375)  \n",
    "Row(year=u'1971', month=u'08', count=375)  \n",
    "Row(year=u'1972', month=u'05', count=375)  \n",
    "Row(year=u'1971', month=u'06', count=374)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684e6ca0",
   "metadata": {},
   "source": [
    "### Q3.Find the average monthly temperature for each available station in Sweden. \n",
    "Your result should include average temperature for each station for each month in the period of 1960-\n",
    "2014. Bear in mind that not every station has the readings for each month in this timeframe. \n",
    "In this exercise you will use the temperature-readings.csv file.\n",
    "\n",
    "The output should contain the following information:\n",
    "\n",
    "year, month, station, avgMonthlyTemperature ORDER BY avgMonthlyTemperature DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3fdb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "sc = SparkContext(appName = 'exercise 3')\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# This path is to the file on hdfs\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "lines = temperature_file.map(lambda line: line.split(\";\"))\n",
    "\n",
    "# (key, value) = (year,month,date,station,temperature)\n",
    "tempReadingsRow = lines.map(lambda x: ( x[1][0:4],x[1][5:7],x[1][8:],x[0] , float(x[3]) ) )\n",
    "\n",
    "## Inferring schema and registering the Dataframe as a table\n",
    "tempReadingsString = [\"year\",\"month\",\"date\",\"station\",\"temperature\"]\n",
    "\n",
    "schemaTempReadings = sqlContext.createDataFrame(tempReadingsRow,tempReadingsString)\n",
    "\n",
    "# Register the DataFrame as a table\n",
    "schemaTempReadings.registerTempTable(\"tempReadingsTable\")\n",
    "\n",
    "#filter years 1950-2014 \n",
    "schemaTempReadings = schemaTempReadings.filter((schemaTempReadings[\"year\"]>= 1960) & (schemaTempReadings[\"year\"]<= 2014))\n",
    "\n",
    "\n",
    "#schemaTempReadings =  schemaTempReadings.select(['year','month','station','temperature'])\n",
    "schemaTempReadingsMinMax = schemaTempReadings.groupBy(\"station\", \"year\", \"month\",\"date\").agg(F.min(\"temperature\").alias(\"min\"),F.max(\"temperature\").alias(\"max\"))\n",
    "\n",
    "schemaTempReadingsMean = schemaTempReadingsMinMax.withColumn(\"Average\", (schemaTempReadingsMinMax[\"max\"] + schemaTempReadingsMinMax[\"min\"]) / 2)\n",
    "\n",
    "schemaTempReadingsMean = schemaTempReadingsMean.groupBy('year', 'month', 'station').agg(F.avg('Average').alias('average_monthly_temperature')).orderBy([\"average_monthly_temperature\"],ascending=False)\n",
    "\n",
    "\n",
    "schemaTempReadingsMean.rdd.saveAsTextFile(\"BDA/output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa435d35",
   "metadata": {},
   "source": [
    "* output of Q3 \n",
    "\n",
    "#### year, month, station, avgMonthlyTemperature ORDER BY avgMonthlyTemperature DESC\n",
    "\n",
    "Row(year=u'2014', month=u'07', station=u'96000', average_monthly_temperature=26.3)  \n",
    "Row(year=u'1994', month=u'07', station=u'96550', average_monthly_temperature=23.071052631578947)  \n",
    "Row(year=u'1983', month=u'08', station=u'54550', average_monthly_temperature=23.0)  \n",
    "Row(year=u'1994', month=u'07', station=u'78140', average_monthly_temperature=22.970967741935485)  \n",
    "Row(year=u'1994', month=u'07', station=u'85280', average_monthly_temperature=22.872580645161296)  \n",
    "Row(year=u'1994', month=u'07', station=u'75120', average_monthly_temperature=22.858064516129037)  \n",
    "Row(year=u'1994', month=u'07', station=u'65450', average_monthly_temperature=22.85645161290323)  \n",
    "Row(year=u'1994', month=u'07', station=u'96000', average_monthly_temperature=22.808064516129033)  \n",
    "Row(year=u'1994', month=u'07', station=u'95160', average_monthly_temperature=22.76451612903226)  \n",
    "Row(year=u'1994', month=u'07', station=u'86200', average_monthly_temperature=22.711290322580645)  \n",
    "Row(year=u'2002', month=u'08', station=u'78140', average_monthly_temperature=22.700000000000003)  \n",
    "Row(year=u'1994', month=u'07', station=u'76000', average_monthly_temperature=22.698387096774198)  \n",
    "Row(year=u'1997', month=u'08', station=u'78140', average_monthly_temperature=22.666129032258066)  \n",
    "Row(year=u'1994', month=u'07', station=u'105260', average_monthly_temperature=22.65967741935484)  \n",
    "Row(year=u'1975', month=u'08', station=u'54550', average_monthly_temperature=22.642857142857142)  \n",
    "Row(year=u'2006', month=u'07', station=u'76530', average_monthly_temperature=22.598387096774204)  \n",
    "Row(year=u'1994', month=u'07', station=u'86330', average_monthly_temperature=22.548387096774192)  \n",
    "Row(year=u'2006', month=u'07', station=u'75120', average_monthly_temperature=22.52741935483871)  \n",
    "Row(year=u'1994', month=u'07', station=u'54300', average_monthly_temperature=22.469354838709677)  \n",
    "Row(year=u'2006', month=u'07', station=u'78140', average_monthly_temperature=22.45806451612903)  \n",
    "Row(year=u'2001', month=u'07', station=u'96550', average_monthly_temperature=22.408333333333335)  \n",
    "Row(year=u'2010', month=u'07', station=u'98180', average_monthly_temperature=22.379032258064516)  \n",
    "Row(year=u'2006', month=u'07', station=u'65450', average_monthly_temperature=22.377419354838707)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f297d",
   "metadata": {},
   "source": [
    "### Q4  Provide a list of stations with their associated maximum measured temperatures and maximum measured daily precipitation.  \n",
    "\n",
    "Show only those stations where the maximum temperature is between 25 and 30 degrees and maximum daily precipitation is between 100 mm and 200mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16debaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sc = SparkContext(appName=\"exercise 1\")\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Read temperature data\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "temperature_lines = temperature_file.map(lambda line: line.split(\";\"))\n",
    "get_temperature = temperature_lines.map(lambda x: (x[0], float(x[3])))\n",
    "tempschema = StructType([\n",
    "    StructField(\"station\", StringType(), True),\n",
    "    StructField(\"temp\", FloatType(), True)\n",
    "])\n",
    "temperature_df = spark.createDataFrame(get_temperature, tempschema)\n",
    "\n",
    "# Calculate max temperature\n",
    "station_max_temp = temperature_df.groupBy(\"station\").agg(F.max(\"temp\").alias(\"temp\"))\n",
    "filter_temp = station_max_temp.filter((station_max_temp[\"temp\"] >= 25) & (station_max_temp[\"temp\"] <= 30))\n",
    "\n",
    "# Read precipitation data\n",
    "precipitation_file = sc.textFile(\"BDA/input/precipitation-readings.csv\")\n",
    "precipitation_lines = precipitation_file.map(lambda line: line.split(\";\"))\n",
    "get_precipitation = precipitation_lines.map(lambda x: (x[0], x[1], float(x[3])))\n",
    "prec_schema = StructType([\n",
    "    StructField(\"station\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"prec\", FloatType(), True)\n",
    "])\n",
    "precipitation_df = spark.createDataFrame(get_precipitation, prec_schema)\n",
    "\n",
    "# Calculate total precipitation per day\n",
    "daily_precipitation = precipitation_df.groupBy(\"station\", \"date\").agg(F.sum(\"prec\").alias(\"total_prec\"))\n",
    "# Get the max precipitation of a station \n",
    "max_precipitation_per_station = daily_precipitation.groupBy(\"station\").agg(F.max(\"total_prec\").alias(\"max_total_prec\"))\n",
    "# Filter precipitation\n",
    "filter_prec = max_precipitation_per_station.filter((max_precipitation_per_station[\"max_total_prec\"] >= 100) & (max_precipitation_per_station[\"max_total_prec\"] <= 200))\n",
    "\n",
    "combine_temp_prec = filter_temp.join(filter_prec.alias('prec'), 'station', 'inner')\n",
    "#output\n",
    "combine_temp_prec_combine = combine_temp_prec.rdd.coalesce(1)\n",
    "filter_temp_combine = combine_temp_prec_combine.sortBy(lambda x: x[0], ascending=False)\n",
    "filter_temp_combine.saveAsTextFile(\"BDA/output/l2_prec_temp\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e850e554",
   "metadata": {},
   "source": [
    "### output of Q4 is empty, since no data meet the criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565d2266",
   "metadata": {},
   "source": [
    "### Q5. Calculate the average monthly precipitation for the Ã–stergotland region \n",
    "(list of stations is\n",
    "provided in the separate file) for the period 1993-2016. In orderto dothis, you will first need to calculate the total monthly precipitation for each station before calculating the monthly\n",
    "average (by averaging over stations).\n",
    "In this exercise you will use the precipitation-readings.csv and stations-Ostergotland.csv\n",
    "files. HINT (not for the SparkSQL lab): Avoid using joins here! stations-Ostergotland.csv is\n",
    "small and if distributed will cause a number of unnecessary shuffles when joined with\n",
    "precipitationRDD. If you distribute precipitation-readings.csv then either repartition your\n",
    "stations RDD to 1 partition or make use of the collect function to acquire a python list and\n",
    "broadcast function to broadcast the list to all nodes.\n",
    "The output should contain the following information:  \n",
    "\n",
    "year, month, avgMonthlyPrecipitation ORDER BY year DESC, month DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146b5a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "sc = SparkContext(appName = 'exercise 5')\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "precipitaion_file = sc.textFile('BDA/input/precipitation-readings.csv')\n",
    "stations_file = sc.textFile('BDA/input/stations-Ostergotland.csv')\n",
    "\n",
    "lines = precipitaion_file.map(lambda line: line.split(';'))\n",
    "stations = stations_file.map(lambda line: line.split(';'))\n",
    "\n",
    "# (key, value) = (year,month,station,precipitation)\n",
    "precipReadingRow = lines.map(lambda x: ( x[1][0:4],x[1][5:7],x[0],float(x[3]) ) )\n",
    "# (key,value) = (year,month,station,\n",
    "stationsReadingRow = stations.map(lambda x: (x[0],x[1],x[2],x[3],x[4],x[5],x[6],x[7]))\n",
    "\n",
    "stationReadingString = ['station','name','height','latitude','longitude','readingfrom','readingto','Elavtion']\n",
    "\n",
    "## Inferring schema and registering the Dataframe as a table\n",
    "precipReadingsString = [\"year\",\"month\",\"station\",\"precipitation\"]\n",
    "\n",
    "schemaPrecipReadings = sqlContext.createDataFrame(precipReadingRow,precipReadingsString)\n",
    "schemaStations = sqlContext.createDataFrame(stationsReadingRow,stationReadingString)\n",
    "\n",
    "# Register the DataFrame as a table\n",
    "schemaPrecipReadings.registerTempTable(\"PrecipReadingsTable\")\n",
    "schemaStations.registerTempTable('StationsTable')\n",
    "\n",
    "schemaStations = schemaStations.select(['station'])\n",
    "\n",
    "#filter years 1993-2016 \n",
    "schemaPrecipReadings = schemaPrecipReadings.filter((schemaPrecipReadings[\"year\"]>= 1993) & (schemaPrecipReadings[\"year\"]<= 2016))\n",
    "\n",
    "#join with station\n",
    "schemaPrecipReadings = schemaPrecipReadings.join(schemaStations,['station'])\n",
    "\n",
    "schemaPrecipReadings =  schemaPrecipReadings.select(['year','month','station','precipitation'])\n",
    "\n",
    "#calculate total monthly precipitation\n",
    "schemaPrecipReadingsMean = schemaPrecipReadings.groupBy('year','month','station').agg(F.sum('precipitation').alias('total')).orderBy(['total'],ascending = False)\n",
    "\n",
    "#average over stations\n",
    "schemaPrecipReadingsMean = schemaPrecipReadingsMean.groupBy('year','month').agg(F.avg('total').alias('avg')).orderBy(['year','month'],ascending = False)\n",
    "\n",
    "\n",
    "schemaPrecipReadingsMean.rdd.saveAsTextFile(\"BDA/output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14fbee1",
   "metadata": {},
   "source": [
    "* Output of Q5\n",
    "\n",
    "#### year, month, avgMonthlyPrecipitation ORDER BY year DESC, month DESC\n",
    "\n",
    "Row(year=u'2016', month=u'07', avg=0.0)  \n",
    "Row(year=u'2016', month=u'06', avg=47.662499999999994)  \n",
    "Row(year=u'2016', month=u'05', avg=29.250000000000004)  \n",
    "Row(year=u'2016', month=u'04', avg=26.900000000000006)  \n",
    "Row(year=u'2016', month=u'03', avg=19.962500000000002)  \n",
    "Row(year=u'2016', month=u'02', avg=21.562500000000004)  \n",
    "Row(year=u'2016', month=u'01', avg=22.325000000000003)  \n",
    "Row(year=u'2015', month=u'12', avg=28.925000000000004)  \n",
    "Row(year=u'2015', month=u'11', avg=63.88750000000002)  \n",
    "Row(year=u'2015', month=u'10', avg=2.2625)  \n",
    "Row(year=u'2015', month=u'09', avg=101.29999999999998)  \n",
    "Row(year=u'2015', month=u'08', avg=26.987499999999997)  \n",
    "Row(year=u'2015', month=u'07', avg=119.09999999999995)  \n",
    "Row(year=u'2015', month=u'06', avg=78.66250000000001)  \n",
    "Row(year=u'2015', month=u'05', avg=93.225)  \n",
    "Row(year=u'2015', month=u'04', avg=15.3375)  \n",
    "Row(year=u'2015', month=u'03', avg=42.612500000000004)  \n",
    "Row(year=u'2015', month=u'02', avg=24.825)  \n",
    "Row(year=u'2015', month=u'01', avg=59.11250000000003)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
