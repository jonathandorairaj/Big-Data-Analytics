{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2012df00-16df-41f6-8cd5-0d602041ca3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "303dae47-8fe3-4a4f-a941-c7d8fe92b537",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220eee25-69ac-4d61-9cd8-e378a994be40",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Q1. What are the lowest and highest temperatures measured each year for the period 1950-2014?\n",
    "Provide the lists sorted in the descending order with respect to the maximum temperature. \n",
    "In this exercise you will use the temperature-readings.csv file.\n",
    "The output should at least contain the following information (You can also include a Station column so that you may find multiple stations that record the highest (lowest)\n",
    "temperature.)\n",
    "\n",
    "year, station with the max, maxValue ORDER BY maxValue DESC  \n",
    "year, station with the min, minValue ORDER BY minValue DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae340a94-8967-4a4e-b6af-1230a373113a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "sc = SparkContext(appName = 'exercise 1')\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#sqlContext = HiveContext(sc)\n",
    "\n",
    "# This path is to the file on hdfs\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "lines = temperature_file.map(lambda line: line.split(\";\"))\n",
    "\n",
    "# (key, value) = (year,temperature)\n",
    "tempReadingsRow = lines.map(lambda p: (p[0],int(p[1].split(\"-\")[0]),float(p[3]) ))\n",
    "\n",
    "## Inferring schema and registering the Dataframe as a table\n",
    "tempReadingsString = [\"station\",\"year\",\"value\"]\n",
    "\n",
    "schemaTempReadings = sqlContext.createDataFrame(tempReadingsRow,tempReadingsString)\n",
    "\n",
    "# Register the DataFrame as a table\n",
    "schemaTempReadings.registerTempTable(\"tempReadingsTable\")\n",
    "\n",
    "#filter years 1950-2014 \n",
    "schemaTempReadings = schemaTempReadings.filter((schemaTempReadings[\"year\"]>= 1950) & (schemaTempReadings[\"year\"]<= 2014))\n",
    "#schemaTempReadings = schemaTempReadings.collect()\n",
    "\n",
    "\n",
    "schemaTempReadings =  schemaTempReadings.select(['year','station','value'])\n",
    "\n",
    "schemaTempReadingsMin = schemaTempReadings.groupBy('year','station').agg(F.min('value').alias('min'))\n",
    "schemaTempReadingsMinYear = schemaTempReadingsMin.groupBy('year').agg(F.min('min').alias('min'))\n",
    "\n",
    "year_station_mintemp = schemaTempReadingsMin.join(schemaTempReadingsMinYear, ['year', 'min']).select('year', 'station', 'min').orderBy(['min'],ascending = False)\n",
    "\n",
    "\n",
    "schemaTempReadingsMax = schemaTempReadings.groupBy('year','station').agg(F.max('value').alias('max'))\n",
    "\n",
    "schemaTempReadingsMaxYear = schemaTempReadingsMax.groupBy('year').agg(F.max('max').alias('max'))\n",
    "\n",
    "year_station_maxtemp = schemaTempReadingsMax.join(schemaTempReadingsMaxYear, ['year', 'max']).select('year', 'station', 'max').orderBy(['max'],ascending = False)\n",
    "\n",
    "year_station_mintemp.rdd.saveAsTextFile(\"BDA/output/q1_min\")\n",
    "year_station_maxtemp.rdd.saveAsTextFile(\"BDA/output/q1_max\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e89dcba-5063-4cdb-8c8a-4099677f083c",
   "metadata": {},
   "source": [
    "* output \n",
    "\n",
    "#### year, station with the max, maxValue ORDER BY maxValue DESC  \n",
    "\n",
    "Row(year=1975, station=u'86200', max=36.1)  \n",
    "Row(year=1992, station=u'63600', max=35.4)  \n",
    "Row(year=1994, station=u'117160', max=34.7)  \n",
    "Row(year=2010, station=u'75250', max=34.4)  \n",
    "Row(year=2014, station=u'96560', max=34.4)  \n",
    "Row(year=1989, station=u'63050', max=33.9)  \n",
    "Row(year=1982, station=u'94050', max=33.8)  \n",
    "Row(year=1968, station=u'137100', max=33.7)  \n",
    "Row(year=1966, station=u'151640', max=33.5)  \n",
    "Row(year=2002, station=u'78290', max=33.3)    \n",
    "Row(year=1983, station=u'98210', max=33.3)   \n",
    "Row(year=1970, station=u'103080', max=33.2)  \n",
    "Row(year=1986, station=u'76470', max=33.2)  \n",
    "Row(year=1956, station=u'145340', max=33.0)  \n",
    "Row(year=2000, station=u'62400', max=33.0)  \n",
    "Row(year=1959, station=u'65160', max=32.8)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad25745-52ec-476f-a19a-bed91771e7fa",
   "metadata": {},
   "source": [
    "#### year, station with the min, minValue ORDER BY minValue DESC\n",
    "\n",
    "Row(year=1990, station=u'147270', min=-35.0)  \n",
    "Row(year=1990, station=u'166870', min=-35.0)  \n",
    "Row(year=1952, station=u'192830', min=-35.5)  \n",
    "Row(year=1974, station=u'166870', min=-35.6)  \n",
    "Row(year=1974, station=u'179950', min=-35.6)  \n",
    "Row(year=1954, station=u'113410', min=-36.0)  \n",
    "Row(year=1992, station=u'179960', min=-36.1)  \n",
    "Row(year=1975, station=u'157860', min=-37.0)   \n",
    "Row(year=1972, station=u'167860', min=-37.5)  \n",
    "Row(year=2000, station=u'169860', min=-37.6)  \n",
    "Row(year=1995, station=u'182910', min=-37.6)  \n",
    "Row(year=1957, station=u'159970', min=-37.8)  \n",
    "Row(year=1983, station=u'191900', min=-38.2)  \n",
    "Row(year=1989, station=u'166870', min=-38.2)  \n",
    "Row(year=1953, station=u'183760', min=-38.4)  \n",
    "Row(year=2009, station=u'179960', min=-38.5)  \n",
    "Row(year=1993, station=u'191900', min=-39.0)  \n",
    "Row(year=1984, station=u'191900', min=-39.2)  \n",
    "Row(year=1984, station=u'123480', min=-39.2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74cb868-e647-4b43-aed6-201b63e0e783",
   "metadata": {},
   "source": [
    "### Q3.Find the average monthly temperature for each available station in Sweden. \n",
    "Your result should include average temperature for each station for each month in the period of 1960-\n",
    "2014. Bear in mind that not every station has the readings for each month in this timeframe. \n",
    "In this exercise you will use the temperature-readings.csv file.\n",
    "\n",
    "The output should contain the following information:\n",
    "\n",
    "year, month, station, avgMonthlyTemperature ORDER BY avgMonthlyTemperature DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608c9e0-c579-4b94-9266-d6b0ee6f770f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "sc = SparkContext(appName = 'exercise 3')\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# This path is to the file on hdfs\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "lines = temperature_file.map(lambda line: line.split(\";\"))\n",
    "\n",
    "# (key, value) = (year,month,date,station,temperature)\n",
    "tempReadingsRow = lines.map(lambda x: ( x[1][0:4],x[1][5:7],x[1][8:],x[0] , float(x[3]) ) )\n",
    "\n",
    "## Inferring schema and registering the Dataframe as a table\n",
    "tempReadingsString = [\"year\",\"month\",\"date\",\"station\",\"temperature\"]\n",
    "\n",
    "schemaTempReadings = sqlContext.createDataFrame(tempReadingsRow,tempReadingsString)\n",
    "\n",
    "# Register the DataFrame as a table\n",
    "schemaTempReadings.registerTempTable(\"tempReadingsTable\")\n",
    "\n",
    "#filter years 1950-2014 \n",
    "schemaTempReadings = schemaTempReadings.filter((schemaTempReadings[\"year\"]>= 1960) & (schemaTempReadings[\"year\"]<= 2014))\n",
    "\n",
    "\n",
    "schemaTempReadings =  schemaTempReadings.select(['year','month','station','temperature'])\n",
    "\n",
    "schemaTempReadingsMean = schemaTempReadings.groupBy('year','month','station').agg(F.mean('temperature').alias('avg')).orderBy(['avg'],ascending = False)\n",
    "\n",
    "schemaTempReadingsMean.rdd.saveAsTextFile(\"BDA/output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321a3892-9905-4e54-8451-919692ef6236",
   "metadata": {},
   "source": [
    "* output of Q3 \n",
    "\n",
    "#### year, month, station, avgMonthlyTemperature ORDER BY avgMonthlyTemperature DESC\n",
    "\n",
    "Row(year=u'2014', month=u'07', station=u'96000', avg=26.3)  \n",
    "Row(year=u'1994', month=u'07', station=u'65450', avg=23.65483870967742)  \n",
    "Row(year=u'1994', month=u'07', station=u'95160', avg=23.505376344086027)  \n",
    "Row(year=u'1994', month=u'07', station=u'75120', avg=23.26881720430107)  \n",
    "Row(year=u'1994', month=u'07', station=u'105260', avg=23.143820224719107)  \n",
    "Row(year=u'1994', month=u'07', station=u'85280', avg=23.108602150537635)  \n",
    "Row(year=u'1983', month=u'08', station=u'54550', avg=23.0)  \n",
    "Row(year=u'1975', month=u'08', station=u'54550', avg=22.9625)  \n",
    "Row(year=u'1994', month=u'07', station=u'96550', avg=22.957894736842114)  \n",
    "Row(year=u'1994', month=u'07', station=u'96000', avg=22.931182795698923)  \n",
    "Row(year=u'1994', month=u'07', station=u'106070', avg=22.822580645161295)  \n",
    "Row(year=u'1972', month=u'07', station=u'173960', avg=22.776666666666667)  \n",
    "Row(year=u'1994', month=u'07', station=u'54300', avg=22.76021505376344)  \n",
    "Row(year=u'1994', month=u'07', station=u'85210', avg=22.755913978494615)  \n",
    "Row(year=u'2006', month=u'07', station=u'65450', avg=22.74086021505376)  \n",
    "Row(year=u'2006', month=u'07', station=u'75120', avg=22.73010752688173)  \n",
    "Row(year=u'1994', month=u'07', station=u'103080', avg=22.708602150537626)  \n",
    "Row(year=u'1994', month=u'07', station=u'92100', avg=22.698924731182792)  \n",
    "Row(year=u'1994', month=u'07', station=u'94180', avg=22.68172043010753)  \n",
    "Row(year=u'1994', month=u'07', station=u'83230', avg=22.577419354838707)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b38265-d26e-436b-975c-dfe37fec4d88",
   "metadata": {},
   "source": [
    "### Q5. Calculate the average monthly precipitation for the Ã–stergotland region \n",
    "(list of stations is\n",
    "provided in the separate file) for the period 1993-2016. In orderto dothis, you will first need to calculate the total monthly precipitation for each station before calculating the monthly\n",
    "average (by averaging over stations).\n",
    "In this exercise you will use the precipitation-readings.csv and stations-Ostergotland.csv\n",
    "files. HINT (not for the SparkSQL lab): Avoid using joins here! stations-Ostergotland.csv is\n",
    "small and if distributed will cause a number of unnecessary shuffles when joined with\n",
    "precipitationRDD. If you distribute precipitation-readings.csv then either repartition your\n",
    "stations RDD to 1 partition or make use of the collect function to acquire a python list and\n",
    "broadcast function to broadcast the list to all nodes.\n",
    "The output should contain the following information:  \n",
    "\n",
    "year, month, avgMonthlyPrecipitation ORDER BY year DESC, month DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6e768-8d9b-487d-8d4a-1b2a89df2247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql import HiveContext\n",
    "\n",
    "sc = SparkContext(appName = 'exercise 5')\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "precipitaion_file = sc.textFile('BDA/input/precipitation-readings.csv')\n",
    "stations_file = sc.textFile('BDA/input/stations-Ostergotland.csv')\n",
    "\n",
    "lines = precipitaion_file.map(lambda line: line.split(';'))\n",
    "stations = stations_file.map(lambda line: line.split(';'))\n",
    "\n",
    "# (key, value) = (year,month,station,precipitation)\n",
    "precipReadingRow = lines.map(lambda x: ( x[1][0:4],x[1][5:7],x[0],float(x[3]) ) )\n",
    "# (key,value) = (year,month,station,\n",
    "stationsReadingRow = stations.map(lambda x: (x[0],x[1],x[2],x[3],x[4],x[5],x[6],x[7]))\n",
    "\n",
    "stationReadingString = ['station','name','height','latitude','longitude','readingfrom','readingto','Elavtion']\n",
    "\n",
    "## Inferring schema and registering the Dataframe as a table\n",
    "precipReadingsString = [\"year\",\"month\",\"station\",\"precipitation\"]\n",
    "\n",
    "schemaPrecipReadings = sqlContext.createDataFrame(precipReadingRow,precipReadingsString)\n",
    "schemaStations = sqlContext.createDataFrame(stationsReadingRow,stationReadingString)\n",
    "\n",
    "# Register the DataFrame as a table\n",
    "schemaPrecipReadings.registerTempTable(\"PrecipReadingsTable\")\n",
    "schemaStations.registerTempTable('StationsTable')\n",
    "\n",
    "schemaStations = schemaStations.select(['station'])\n",
    "\n",
    "#filter years 1993-2016 \n",
    "schemaPrecipReadings = schemaPrecipReadings.filter((schemaPrecipReadings[\"year\"]>= 1993) & (schemaPrecipReadings[\"year\"]<= 2016))\n",
    "\n",
    "#join with station\n",
    "schemaPrecipReadings = schemaPrecipReadings.join(schemaStations,['station'])\n",
    "\n",
    "schemaPrecipReadings =  schemaPrecipReadings.select(['year','month','station','precipitation'])\n",
    "\n",
    "#calculate total monthly precipitation\n",
    "schemaPrecipReadingsMean = schemaPrecipReadings.groupBy('year','month','station').agg(F.sum('precipitation').alias('total')).orderBy(['total'],ascending = False)\n",
    "\n",
    "#average over stations\n",
    "schemaPrecipReadingsMean = schemaPrecipReadingsMean.groupBy('year','month').agg(F.avg('total').alias('avg')).orderBy(['year','month'],ascending = False)\n",
    "\n",
    "\n",
    "schemaPrecipReadingsMean.rdd.saveAsTextFile(\"BDA/output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5292f60-6e72-41a9-9b7c-3d909e2cd129",
   "metadata": {},
   "source": [
    "* Output of Q5\n",
    "\n",
    "#### year, month, avgMonthlyPrecipitation ORDER BY year DESC, month DESC\n",
    "\n",
    "Row(year=u'2016', month=u'07', avg=0.0)  \n",
    "Row(year=u'2016', month=u'06', avg=47.662499999999994)  \n",
    "Row(year=u'2016', month=u'05', avg=29.250000000000004)  \n",
    "Row(year=u'2016', month=u'04', avg=26.900000000000006)  \n",
    "Row(year=u'2016', month=u'03', avg=19.962500000000002)  \n",
    "Row(year=u'2016', month=u'02', avg=21.562500000000004)  \n",
    "Row(year=u'2016', month=u'01', avg=22.325000000000003)  \n",
    "Row(year=u'2015', month=u'12', avg=28.925000000000004)  \n",
    "Row(year=u'2015', month=u'11', avg=63.88750000000002)  \n",
    "Row(year=u'2015', month=u'10', avg=2.2625)  \n",
    "Row(year=u'2015', month=u'09', avg=101.29999999999998)  \n",
    "Row(year=u'2015', month=u'08', avg=26.987499999999997)  \n",
    "Row(year=u'2015', month=u'07', avg=119.09999999999995)  \n",
    "Row(year=u'2015', month=u'06', avg=78.66250000000001)  \n",
    "Row(year=u'2015', month=u'05', avg=93.225)  \n",
    "Row(year=u'2015', month=u'04', avg=15.3375)  \n",
    "Row(year=u'2015', month=u'03', avg=42.612500000000004)  \n",
    "Row(year=u'2015', month=u'02', avg=24.825)  \n",
    "Row(year=u'2015', month=u'01', avg=59.11250000000003)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211a2b13-b089-4e6d-8c4b-871e0211dc09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
