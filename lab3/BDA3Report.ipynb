{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29932426",
   "metadata": {},
   "source": [
    "# LAB EXERCISE 3: MACHINE LEARNING  \n",
    "Implement in Spark (PySpark) a kernel model to predict the hourly temperatures for a date\n",
    "and place in Sweden. To do so, you should use the files temperature-readings.csv and\n",
    "stations.csv from previous labs. Specifically, the forecast should consist of the predicted\n",
    "temperatures from 4 am to 24 pm in an interval of 2 hours for a date and place in Sweden.\n",
    "Use a kernel that is the sum of three Gaussian kernels:  \n",
    "● The first to account for the distance from a station to the point of interest.  \n",
    "● The second to account for the distance between the day a temperature measurement was made and the day of interest.  \n",
    "● The third to account for the distance between the hour of the day a temperature measurement was made and the hour of interest.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fbaf19",
   "metadata": {},
   "source": [
    "## Kernel Model\n",
    "#### Note:\n",
    "For hour interval we  use [0:00:00, 04:00:00, 06:00:00 ...] and not 24:00:00, this is because in temperature-reading.csv, temperature is recorded from 00:00:00, so we keep that for consistancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from math import radians, cos, sin, asin, sqrt, exp\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.broadcast import Broadcast\n",
    "import numpy as np\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "def day_difference(day1, day2):\n",
    "    diff = abs(datetime.strptime(str(day1), \"%Y-%m-%d\") - datetime.strptime(str(day2), \"%Y-%m-%d\"))\n",
    "    no_days = diff.days\n",
    "    return no_days\n",
    "\n",
    "def hour_diff(time1, time2):\n",
    "    diff = abs(datetime.strptime(time1, \"%H:%M:%S\") - datetime.strptime(time2, \"%H:%M:%S\"))\n",
    "    diff = (diff.total_seconds()) / 3600\n",
    "    return diff\n",
    "\n",
    "def gaussian_kernel(u, h):\n",
    "    return np.exp(-u**2 / (2 * h**2))\n",
    "\n",
    "def sum_kernel(distance_kernel, day_kernel, time_kernel):\n",
    "    res = distance_kernel + day_kernel + time_kernel\n",
    "    return res\n",
    "\n",
    "def product_kernel(distance_kernel, day_kernel, time_kernel):\n",
    "    res = distance_kernel * day_kernel * time_kernel\n",
    "    return res\n",
    "\n",
    "# Value to predict\n",
    "target_latitude = 58.68681\n",
    "target_longitude = 15.92183\n",
    "target_date = '2014-05-17'\n",
    "hour_list= [\"00:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\", \"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]\n",
    "\n",
    "\n",
    "# Kernel width\n",
    "h_dist = 100\n",
    "h_day = 15\n",
    "h_time = 5\n",
    "\n",
    "# Create SparkContext\n",
    "sc = SparkContext(appName=\"Lab 3 ML\")\n",
    "\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "station_file = sc.textFile(\"BDA/input/stations.csv\")\n",
    "\n",
    "temperature_data = temperature_file.map(lambda x: x.split(';'))\n",
    "stations_data = station_file.map(lambda x: x.split(';'))\n",
    "\n",
    "#Filter the previous data, this can save some computation\n",
    "target_date_strip = datetime.strptime(target_date, '%Y-%m-%d')\n",
    "prev_temp = temperature_data.filter(lambda x: datetime.strptime(x[1], '%Y-%m-%d') < target_date_strip)\n",
    "\n",
    "# pre-calculate station_distkernel and broadcast it (to speed up process)\n",
    "station_distkernel = stations_data.map(lambda x: (x[0], gaussian_kernel(haversine(target_longitude, target_latitude, float(x[4]), float(x[3])), h_dist))).collectAsMap()\n",
    "broadcast_station_distkernel = sc.broadcast(station_distkernel)\n",
    "\n",
    "#station id, time, temp, day_kernel, cache it to save time\n",
    "temperature_data_datekernel = prev_temp.map(lambda x: (x[0], x[2], x[3], gaussian_kernel(day_difference(target_date, x[1]), h_day))).cache()\n",
    "\n",
    "\n",
    "predictions = {}\n",
    "for hour in hour_list:\n",
    "    #temp, dist_kernel, day_kernel, hour_kernel\n",
    "    temp_kernels = temperature_data_datekernel.map(lambda x: (float(x[2]),\n",
    "                                                broadcast_station_distkernel.value[x[0]],\n",
    "                                                x[3],\n",
    "                                                gaussian_kernel(hour_diff(hour, x[1]), h_time)))\n",
    "    #temp,sumkernel, prodkernel                                \n",
    "    temp_both_kernels = temp_kernels.map(lambda x: (x[0], sum_kernel(x[1], x[2], x[3]), product_kernel(x[1], x[2], x[3])))\n",
    "\n",
    "    #sum_kernel, sum_kernel*temp, prod_kernel, prod_kernel*temp \n",
    "    cal_kerneltemp = temp_both_kernels.map(lambda x :(x[1],x[0]*x[1],x[2],x[0]*x[2]))\n",
    "    \n",
    "    #sum all the elements\n",
    "    sum_kerneltemp = cal_kerneltemp.reduce(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2], x[3] + y[3]))\n",
    "    \n",
    "    #sum_prediction,prod_prediction\n",
    "    predictions[hour]=(sum_kerneltemp[1]/sum_kerneltemp[0],sum_kerneltemp[3]/sum_kerneltemp[2])\n",
    "\n",
    "# Convert predictions dictionary to RDD\n",
    "predictions_rdd = sc.parallelize(list(predictions.items()))\n",
    "predictions_rdd = predictions_rdd.coalesce(1)\n",
    "predictions_rdd = predictions_rdd.sortByKey()\n",
    "# Save the RDD as text files\n",
    "predictions_rdd.saveAsTextFile(\"BDA/output/predictions\")                                                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704efb90",
   "metadata": {},
   "source": [
    "## Time(Hour),Prediction using summation kernel, Prediction using Prodoct kernel\n",
    "('00:00:00', (4.2202401242697389, 5.2914482209815956))  \n",
    "('04:00:00', (4.3552953714587552, 6.3959774649343277))  \n",
    "('06:00:00', (4.6245740528807513, 7.1339973346514123))  \n",
    "('08:00:00', (4.9627767000765868, 7.9163683859293865))  \n",
    "('10:00:00', (5.2939137306831405, 8.6202196406263436))  \n",
    "('12:00:00', (5.5383779571661025, 9.1060230972701959))  \n",
    "('14:00:00', (5.6461283024140405, 9.2821452649169416))  \n",
    "('16:00:00', (5.6191253696649843, 9.1510902056699788))  \n",
    "('18:00:00', (5.5036901822620719, 8.7955867007308655))  \n",
    "('20:00:00', (5.3640549651923441, 8.3245303183460653))  \n",
    "('22:00:00', (5.2613166212619547, 7.8280808797063388))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00543ed",
   "metadata": {},
   "source": [
    "## MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f831f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from datetime import datetime\n",
    "from math import radians, cos, sin, asin, sqrt, exp\n",
    "from pyspark.broadcast import Broadcast\n",
    "\n",
    "# For data handling, the plan is as below\n",
    "# Use the distance difference from the geographical center of sweden\n",
    "# Use how many dates have passed since 1950/01/01\n",
    "# Use the hour difference from 00:00:00\n",
    "# Which means the features are distance,day_diff, hour_diff, \n",
    "\n",
    "# Function to calculate haversine distance\n",
    "def haversine(lon1, lat1, lon2=16.321998712, lat2=62.38583179): #geographical center of sweden\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "def day_diff(day1, day2=\"1950-01-01\"):\n",
    "    diff = abs(datetime.strptime(str(day1), \"%Y-%m-%d\") - datetime.strptime(str(day2), \"%Y-%m-%d\"))\n",
    "    no_days = diff.days\n",
    "    return no_days\n",
    "\n",
    "def hour_diff(time1, time2=\"00:00:00\"):\n",
    "    diff = abs(datetime.strptime(time1, \"%H:%M:%S\") - datetime.strptime(time2, \"%H:%M:%S\"))\n",
    "    diff = (diff.total_seconds()) / 3600\n",
    "    return diff\n",
    "\n",
    "\n",
    "sc = SparkContext(appName=\"Lab 3 ML\")\n",
    "target_date = '2014-5-17'\n",
    "target_latitude = 58.68681\n",
    "target_longitude = 15.92183\n",
    "\n",
    "target_distance = haversine(lon1=target_longitude, lat1=target_latitude) \n",
    "target_date_diff = day_diff(day1=target_date)\n",
    "\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "#temperature_file = temperature_file.sample(withReplacement=False, fraction=0.1)\n",
    "temperature_data = temperature_file.map(lambda x: x.split(';'))\n",
    "\n",
    "#filter out data after target date\n",
    "target_date_strip = datetime.strptime(target_date, '%Y-%m-%d')\n",
    "prev_temp = temperature_data.filter(lambda x: datetime.strptime(x[1], '%Y-%m-%d') < target_date_strip)\n",
    "\n",
    "\n",
    "station_file = sc.textFile(\"BDA/input/stations.csv\")\n",
    "stations_data = station_file.map(lambda x: x.split(';'))\n",
    "\n",
    "#Same for kernel model, broadcast distance for faster access\n",
    "stations_distance = stations_data.map(lambda x: (x[0], haversine(lat1=float(x[3]), lon1=float(x[4])))).collectAsMap()\n",
    "broadcast_stations_distance = sc.broadcast(stations_distance)\n",
    "\n",
    "training_temp = prev_temp.map(lambda x: (\n",
    "    float(x[3]), day_diff(x[1]), hour_diff(x[2]), broadcast_stations_distance.value[x[0]]))\n",
    "\n",
    "#standardized\n",
    "features = training_temp.map(lambda x: x[1:])\n",
    "standardizer = StandardScaler()\n",
    "model = standardizer.fit(features)\n",
    "features_transform = model.transform(features)\n",
    "label = training_temp.map(lambda x: x[0])\n",
    "standardized_data = label.zip(features_transform)\n",
    "#create training data with standardized features\n",
    "train_data = standardized_data.map(lambda x: LabeledPoint(x[0], [x[1]]))\n",
    "\n",
    "##Create models##\n",
    "\n",
    "#Here the l2 regulation, step_size, iterations are set to avoid huge weight\n",
    "lr_model = LinearRegressionWithSGD.train(train_data, regType='l2', step=0.25, iterations = 50) \n",
    "\n",
    "#Here, maxDepth is set to increase the split of model\n",
    "dt_model = DecisionTree.trainRegressor(train_data, categoricalFeaturesInfo={}, maxDepth= 15)\n",
    "\n",
    "\n",
    "# Create 2 hours interval target, and make it a rdd\n",
    "hour_list = [\"00:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\",\n",
    "             \"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]\n",
    "\n",
    "target_features = []\n",
    "for hour in hour_list:\n",
    "    target_hour = hour_diff(time1=hour)\n",
    "    target_feature = Vectors.dense([float(target_date_diff), target_hour, target_distance])\n",
    "    target_features.append(target_feature)\n",
    "target_features_rdd = sc.parallelize(target_features)\n",
    "\n",
    "\n",
    "# Standardized target features, this is important, since the training features are also standardized\n",
    "target_features_transform = model.transform(target_features_rdd)\n",
    "standardized_target_features = target_features_transform.collect()\n",
    "\n",
    "\n",
    "\n",
    "# Predictions\n",
    "lr_predictions = [lr_model.predict(feature) for feature in standardized_target_features]\n",
    "dt_predictions = [dt_model.predict(feature) for feature in standardized_target_features]\n",
    "\n",
    "\n",
    "\n",
    "# save the predictions to a text file\n",
    "lr_predictions = zip(hour_list, lr_predictions)\n",
    "lr_predictions = sc.parallelize(lr_predictions)\n",
    "lr_predictions = lr_predictions.coalesce(1).sortByKey()\n",
    "lr_predictions.saveAsTextFile(\"BDA/output/lr_prediction\")\n",
    "\n",
    "dt_predictions = zip(hour_list, dt_predictions)\n",
    "dt_predictions = sc.parallelize(dt_predictions)\n",
    "dt_predictions = dt_predictions.coalesce(1).sortByKey()\n",
    "dt_predictions.saveAsTextFile(\"BDA/output/dt_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf426f",
   "metadata": {},
   "source": [
    "## Output of linear regression    \n",
    "('00:00:00', 3.1035369255720253)  \n",
    "('04:00:00', 3.6499689072471697)  \n",
    "('06:00:00', 3.9231848980847417)  \n",
    "('08:00:00', 4.1964008889223141)  \n",
    "('10:00:00', 4.4696168797598865)  \n",
    "('12:00:00', 4.7428328705974581)  \n",
    "('14:00:00', 5.0160488614350296)  \n",
    "('16:00:00', 5.289264852272602)  \n",
    "('18:00:00', 5.5624808431101744)  \n",
    "('20:00:00', 5.8356968339477469)  \n",
    "('22:00:00', 6.1089128247853193)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5280253",
   "metadata": {},
   "source": [
    "## Output of Decision Tree  \n",
    "('00:00:00', 4.891478190630056)  \n",
    "('04:00:00', 4.635169183558103)  \n",
    "('06:00:00', 5.317851639885542)  \n",
    "('08:00:00', 5.983913995031351)  \n",
    "('10:00:00', 7.989802623632015)  \n",
    "('12:00:00', 7.989802623632015)  \n",
    "('14:00:00', 7.989802623632015)  \n",
    "('16:00:00', 7.989802623632015)  \n",
    "('18:00:00', 6.873959938366718)  \n",
    "('20:00:00', 4.760130335412936)  \n",
    "('22:00:00', 5.371705739692806)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f7c9f",
   "metadata": {},
   "source": [
    "### QUESTIONS\n",
    "● Show that your choice for the kernels’ width is sensible, i.e. it gives more weight to\n",
    "closer points. Discuss why your definition of closeness is reasonable.  \n",
    "● Repeat the exercise using a kernel that is the product of the three Gaussian kernels\n",
    "above. Compare the results with those obtained for the additive kernel. If they differ,\n",
    "explain why.  \n",
    "● Repeat the exercise using at least two MLlib library models to predict the hourly temperatures for a date and place in Sweden. Compare the results with two Gaussian\n",
    "kernels. If they differ, explain why.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52989994",
   "metadata": {},
   "source": [
    "### Answer  \n",
    "#### Ans1. \n",
    "We choose below, kernels' width  \n",
    "h_dist = 100 km\n",
    "h_day = 15 days\n",
    "h_time = 5 hours\n",
    "Since the weather changes greatly across large distances. So we set the kernel width for distance as 100 km, which give larger weight to nearby station that is(< 100km).  \n",
    "\n",
    "For date kernel, since the temperature will varies from different seasons, and we will like to keep the larger weight to the observation closer to the target date. However, it is important to note that, this will also decrease the weight of the same date from previous year, but for simplicity, we keep it as 15  \n",
    "\n",
    "For time(hour) kernel, since we set the target location in a random place in Östergötlands län, and we experient  large fluctuations of temperature from day and night recently. Hence, we choose 5 hour as our kernel's width to put more emphasize to closer hour.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d33a5",
   "metadata": {},
   "source": [
    "#### Ans2.   \n",
    "By observing our prediction, we can see that the prediction using summation model is lower and also does not vary much compare to multiplication model. This is due to the difference of combanation\n",
    "For example, if there is a observation that is at the same hour, one day before target date, but 1000km away.\n",
    "Using summation model it will still result in large weight because of hour and date. On the other hand, using multiplication the small weight of distance will keep the overall weight down. We will say that the multiplication model is more ideal because we need to consider all three kernels when doing prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b3f91",
   "metadata": {},
   "source": [
    "#### Ans3.\n",
    "The first model we use LinearRegression(LR) with L2 Regulation (RidgeRegression), the using of regulation is to get smaller and more balanced coefficients since we experience unbalanced coefficients when using default LinearRegression.  \n",
    "The Second model we use is Decision Tree regression since it is usable to capture non-linear relationship between variables.\n",
    "\n",
    "In terms of result, we will say that LR model did not do the prediction well, as the temperature around noon is lower than night time. This might because the amount of data for different hour is not balanced so the coefficient is not good.  \n",
    "  \n",
    "For Decision Tree(DT), the time of higher and lower temperature are as expected. However, it will output same value for different hour. We suspect this is due to the depth of the model, but increase the max_depth will further increase run time.  \n",
    "  \n",
    "Overall, we will say that DT gives more accurate prediction than LR, but both not perfect. Also, the way we handle data (by giving reference starting point for location and date) have make our MLlib model result different to Gaussian kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1640352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
