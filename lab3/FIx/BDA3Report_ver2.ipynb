{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29932426",
   "metadata": {},
   "source": [
    "# LAB EXERCISE 3: MACHINE LEARNING  \n",
    "### Yi-Hung Chen (yihch883) Jonathan Dorairaj (jondo380)\n",
    "\n",
    "\n",
    "Implement in Spark (PySpark) a kernel model to predict the hourly temperatures for a date\n",
    "and place in Sweden. To do so, you should use the files temperature-readings.csv and\n",
    "stations.csv from previous labs. Specifically, the forecast should consist of the predicted\n",
    "temperatures from 4 am to 24 pm in an interval of 2 hours for a date and place in Sweden.\n",
    "Use a kernel that is the sum of three Gaussian kernels:  \n",
    "● The first to account for the distance from a station to the point of interest.  \n",
    "● The second to account for the distance between the day a temperature measurement was made and the day of interest.  \n",
    "● The third to account for the distance between the hour of the day a temperature measurement was made and the hour of interest.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fbaf19",
   "metadata": {},
   "source": [
    "## Kernel Model\n",
    "#### Note:\n",
    "For hour interval we  use [0:00:00, 04:00:00, 06:00:00 ...] and not 24:00:00, this is because in temperature-reading.csv, temperature is recorded from 00:00:00, so we keep that for consistancy.\n",
    "\n",
    "For the changes of improvement, we now has first include all data include target_date, and within the loop we discard the data that is in the target_date but later then target hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from math import radians, cos, sin, asin, sqrt, exp\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.broadcast import Broadcast\n",
    "import numpy as np\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "def day_difference(day1, day2):\n",
    "    diff = abs(datetime.strptime(str(day1), \"%Y-%m-%d\") - datetime.strptime(str(day2), \"%Y-%m-%d\"))\n",
    "    no_days = diff.days\n",
    "    return no_days\n",
    "\n",
    "def hour_diff(time1, time2):\n",
    "    diff = abs(datetime.strptime(time1, \"%H:%M:%S\") - datetime.strptime(time2, \"%H:%M:%S\"))\n",
    "    diff = (diff.total_seconds()) / 3600\n",
    "    return diff\n",
    "\n",
    "def gaussian_kernel(u, h):\n",
    "    return np.exp(-u**2 / (2 * h**2))\n",
    "\n",
    "def sum_kernel(distance_kernel, day_kernel, time_kernel):\n",
    "    res = distance_kernel + day_kernel + time_kernel\n",
    "    return res\n",
    "\n",
    "def product_kernel(distance_kernel, day_kernel, time_kernel):\n",
    "    res = distance_kernel * day_kernel * time_kernel\n",
    "    return res\n",
    "\n",
    "# Value to predict\n",
    "target_latitude = 58.68681\n",
    "target_longitude = 15.92183\n",
    "target_date = '2014-05-17'\n",
    "hour_list= [\"00:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\", \"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]\n",
    "\n",
    "\n",
    "# Kernel width\n",
    "h_dist = 100\n",
    "h_day = 15\n",
    "h_time = 5\n",
    "\n",
    "# Create SparkContext\n",
    "sc = SparkContext(appName=\"Lab 3 ML\")\n",
    "\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "station_file = sc.textFile(\"BDA/input/stations.csv\")\n",
    "\n",
    "temperature_data = temperature_file.map(lambda x: x.split(';'))\n",
    "stations_data = station_file.map(lambda x: x.split(';'))\n",
    "\n",
    "#Filter the previous data until target_date, this can save some computation\n",
    "target_date_strip = datetime.strptime(target_date, '%Y-%m-%d')\n",
    "prev_temp = temperature_data.filter(lambda x: datetime.strptime(x[1], '%Y-%m-%d') <= target_date_strip)\n",
    "\n",
    "# pre-calculate station_distkernel and broadcast it (to speed up process)\n",
    "station_distkernel = stations_data.map(lambda x: (x[0], gaussian_kernel(haversine(target_longitude, target_latitude, float(x[4]), float(x[3])), h_dist))).collectAsMap()\n",
    "broadcast_station_distkernel = sc.broadcast(station_distkernel)\n",
    "\n",
    "#station id, hour, temp, day_kernel,date, cache it to save time\n",
    "temperature_data_datekernel = prev_temp.map(lambda x: (x[0], x[2], x[3], gaussian_kernel(day_difference(target_date, x[1]), h_day),x[1])).cache()\n",
    "\n",
    "\n",
    "predictions = {}\n",
    "for hour in hour_list:\n",
    "    target_hour_strip = datetime.strptime(hour, '%H:%M:%S')\n",
    "    #filter: here, it will keep data from previous date, and current date before current hour. Using \"or\" is crucial.\n",
    "    temp_filtered = temperature_data_datekernel.filter(lambda x: datetime.strptime(x[1], '%H:%M:%S') <= target_hour_strip \n",
    "                                                       or datetime.strptime(x[4], '%Y-%m-%d') < target_date_strip)\n",
    "\n",
    "    #temp, dist_kernel, day_kernel, hour_kernel\n",
    "    temp_kernels = temp_filtered.map(lambda x: (float(x[2]),\n",
    "                                                broadcast_station_distkernel.value[x[0]],\n",
    "                                                x[3],\n",
    "                                                gaussian_kernel(hour_diff(hour, x[1]), h_time)))\n",
    "    #temp,sumkernel, prodkernel                                \n",
    "    temp_both_kernels = temp_kernels.map(lambda x: (x[0], sum_kernel(x[1], x[2], x[3]), product_kernel(x[1], x[2], x[3])))\n",
    "\n",
    "    #sum_kernel, sum_kernel*temp, prod_kernel, prod_kernel*temp \n",
    "    cal_kerneltemp = temp_both_kernels.map(lambda x :(x[1],x[0]*x[1],x[2],x[0]*x[2]))\n",
    "    \n",
    "    #sum all the elements\n",
    "    sum_kerneltemp = cal_kerneltemp.reduce(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2], x[3] + y[3]))\n",
    "    \n",
    "    #sum_prediction,prod_prediction\n",
    "    predictions[hour]=(sum_kerneltemp[1]/sum_kerneltemp[0],sum_kerneltemp[3]/sum_kerneltemp[2])\n",
    "\n",
    "# Convert predictions dictionary to RDD\n",
    "predictions_rdd = sc.parallelize(list(predictions.items()))\n",
    "predictions_rdd = predictions_rdd.coalesce(1)\n",
    "predictions_rdd = predictions_rdd.sortByKey()\n",
    "# Save the RDD as text files\n",
    "predictions_rdd.saveAsTextFile(\"BDA/output/predictions\")                                                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704efb90",
   "metadata": {},
   "source": [
    "## Time(Hour),Prediction using summation kernel, Prediction using Prodoct kernel  \n",
    "('00:00:00', (4.2202706240460373, 5.3158347309906233))  \n",
    "('04:00:00', (4.3553966731824358, 6.4255932055837439))  \n",
    "('06:00:00', (4.6247919485850693, 7.1948198019356946))  \n",
    "('08:00:00', (4.9631010258454413, 8.0067912454985475))  \n",
    "('10:00:00', (5.2943558025072575, 8.7488327291660362))  \n",
    "('12:00:00', (5.5389561819903044, 9.2817195697529904))  \n",
    "('14:00:00', (5.6468414622366607, 9.4996309673011758))  \n",
    "('16:00:00', (5.6199946175495556, 9.4069418318349722))  \n",
    "('18:00:00', (5.504789343885677, 9.103157421362587))  \n",
    "('20:00:00', (5.3653601048579729, 8.6470786196598564))  \n",
    "('22:00:00', (5.2628633694851175, 8.1518370736988146))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00543ed",
   "metadata": {},
   "source": [
    "## MLlib  \n",
    "NOTE: For improvement of Mllib assignment, we are asked to re-train the model everytime for differnt hour. Here, like in kernel model, we decide to train on all availabe data from previous day and also the data from the target_date before target_hour. The reason here is we think it is more reasonable to train on all availabel data for comparison, and also using filter for hour before 00:00:00 will result in empty RDD, and it could not be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f831f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from datetime import datetime\n",
    "from math import radians, cos, sin, asin, sqrt, exp\n",
    "from pyspark.broadcast import Broadcast\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "# For data handling, the plan is as below\n",
    "# Use the distance difference from the geographical center of sweden\n",
    "# Use how many dates have passed since 1950/01/01\n",
    "# Use the hour difference from 00:00:00\n",
    "# Which means the features are distance,day_diff, hour_diff \n",
    "\n",
    "# Function to calculate haversine distance\n",
    "def haversine(lon1, lat1, lon2=16.321998712, lat2=62.38583179): #geographical center of Sweden\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "def day_diff(day1, day2=\"1950-01-01\"):\n",
    "    diff = abs(datetime.strptime(str(day1), \"%Y-%m-%d\") - datetime.strptime(str(day2), \"%Y-%m-%d\"))\n",
    "    no_days = diff.days\n",
    "    return no_days\n",
    "\n",
    "def hour_diff(time1, time2=\"00:00:00\"):\n",
    "    diff = abs(datetime.strptime(time1, \"%H:%M:%S\") - datetime.strptime(time2, \"%H:%M:%S\"))\n",
    "    diff = (diff.total_seconds()) / 3600\n",
    "    return diff\n",
    "\n",
    "\n",
    "sc = SparkContext(appName=\"Lab 3 ML\")\n",
    "target_date = '2014-5-17'\n",
    "target_latitude = 58.68681\n",
    "target_longitude = 15.92183\n",
    "\n",
    "target_distance = haversine(lon1=target_longitude, lat1=target_latitude) \n",
    "target_date_diff = day_diff(day1=target_date)\n",
    "\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "temperature_data = temperature_file.map(lambda x: x.split(';'))\n",
    "\n",
    "#filter out data \n",
    "target_date_strip = datetime.strptime(target_date, '%Y-%m-%d')\n",
    "prev_temp = temperature_data.filter(lambda x: datetime.strptime(x[1], '%Y-%m-%d') <= target_date_strip)\n",
    "\n",
    "\n",
    "station_file = sc.textFile(\"BDA/input/stations.csv\")\n",
    "stations_data = station_file.map(lambda x: x.split(';'))\n",
    "\n",
    "#Same for kernel model, broadcast distance for faster access\n",
    "broadcast_stations_distance = sc.broadcast(stations_data.map(lambda x: (x[0], haversine(lat1=float(x[3]), \n",
    "                                                                                        lon1=float(x[4])))).collectAsMap())\n",
    "\n",
    "\n",
    "#temp,day_difference, hour_difference,station_difference\n",
    "training_temp = prev_temp.map(lambda x: (\n",
    "    float(x[3]), day_diff(x[1]), hour_diff(x[2]), broadcast_stations_distance.value[x[0]]))\n",
    "\n",
    "#standardized\n",
    "features = training_temp.map(lambda x: x[1:])\n",
    "standardizer = StandardScaler()\n",
    "model = standardizer.fit(features)\n",
    "features_transform = model.transform(features)\n",
    "label = training_temp.map(lambda x: x[0])\n",
    "standardized_data = label.zip(features_transform)\n",
    "#create training data with standardized features\n",
    "train_data = standardized_data.map(lambda x: LabeledPoint(x[0], [x[1]])).cache()\n",
    "#has the form [LabeledPoint(6.8, [407.396549514,0.998736575617,0.0])]\n",
    "\n",
    "\n",
    "# Create 2 hours interval\n",
    "hour_list = [\"00:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\",\n",
    "             \"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]\n",
    "\n",
    "prediction = {}\n",
    "for hour in hour_list:\n",
    "    target_hour = hour_diff(time1=hour)\n",
    "    target_feature = Vectors.dense([float(target_date_diff), target_hour, target_distance])\n",
    "    target_features_rdd = sc.parallelize([target_feature])\n",
    "    standardized_target_features = model.transform(target_features_rdd)\n",
    "\n",
    "    #calculate the threshold to filter out the data, since the train_data is already normalized\n",
    "    #so here the threshold is also normalized\n",
    "    hour_threshold = standardized_target_features.first()[1]\n",
    "    date_threshold = standardized_target_features.first()[0]\n",
    "    \n",
    "    ##Create models##\n",
    "    #Like in Kernel model, using filter to keep data that is from previous day plus the data from target date before current hour\n",
    "    current_train_data = train_data.filter(lambda x: x.features[1] < hour_threshold or x.features[0] < date_threshold)\n",
    "\n",
    "    dt_model = DecisionTree.trainRegressor(current_train_data, categoricalFeaturesInfo={}, maxDepth= 2)\n",
    "    rf_model = RandomForest.trainRegressor(current_train_data, categoricalFeaturesInfo={}, numTrees=2, maxDepth = 2, maxBins= 4)\n",
    "    \n",
    "    dt_predictions = dt_model.predict(standardized_target_features).collect()[0] \n",
    "    rf_predictions = rf_model.predict(standardized_target_features).collect()[0]\n",
    "\n",
    " \n",
    "    prediction[hour]=(rf_predictions,dt_predictions)\n",
    "       \n",
    "\n",
    "sc.parallelize(prediction.items()).coalesce(1).sortByKey().saveAsTextFile(\"BDA/output/prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbf426f",
   "metadata": {},
   "source": [
    "## Output of RandomForeset, DecisionTree   \n",
    "('00:00:00', (3.0985588718032466, 4.113541626536199))  \n",
    "('04:00:00', (3.4251056247749587, 2.943337272515525))  \n",
    "('06:00:00', (0.6748955899868561, 3.6775034998326688))  \n",
    "('08:00:00', (5.781932868414285, 5.141295977786772))  \n",
    "('10:00:00', (6.104406643287962, 7.721845048955009))  \n",
    "('12:00:00', (5.827991294663879, 7.755180135712269))  \n",
    "('14:00:00', (5.879698886679979, 7.728443930850527))  \n",
    "('16:00:00', (6.387115848117571, 6.625958810394981))  \n",
    "('18:00:00', (5.243456398157547, 5.2804620059489435))  \n",
    "('20:00:00', (4.860529197709121, 4.791182468908703))  \n",
    "('22:00:00', (3.9453976616173683, 4.797614388221039))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f7c9f",
   "metadata": {},
   "source": [
    "### QUESTIONS\n",
    "● Show that your choice for the kernels’ width is sensible, i.e. it gives more weight to\n",
    "closer points. Discuss why your definition of closeness is reasonable.  \n",
    "● Repeat the exercise using a kernel that is the product of the three Gaussian kernels\n",
    "above. Compare the results with those obtained for the additive kernel. If they differ,\n",
    "explain why.  \n",
    "● Repeat the exercise using at least two MLlib library models to predict the hourly temperatures for a date and place in Sweden. Compare the results with two Gaussian\n",
    "kernels. If they differ, explain why.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52989994",
   "metadata": {},
   "source": [
    "### Answer  \n",
    "#### Ans1 and Ans2 do not change after the updated code.\n",
    "#### Ans1. \n",
    "We choose below, kernels' width  \n",
    "h_dist = 100 km\n",
    "h_day = 15 days\n",
    "h_time = 5 hours\n",
    "Since the weather changes greatly across large distances. So we set the kernel width for distance as 100 km, which give larger weight to nearby station that is(< 100km).  \n",
    "\n",
    "For date kernel, since the temperature will varies from different seasons, and we will like to keep the larger weight to the observation closer to the target date. However, it is important to note that, this will also decrease the weight of the same date from previous year, but for simplicity, we keep it as 15  \n",
    "\n",
    "For time(hour) kernel, since we set the target location in a random place in Östergötlands län, and we experient  large fluctuations of temperature from day and night recently. Hence, we choose 5 hour as our kernel's width to put more emphasize to closer hour.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d33a5",
   "metadata": {},
   "source": [
    "#### Ans2.   \n",
    "By observing our prediction, we can see that the prediction using summation model is lower and also does not vary much compare to multiplication model. This is due to the difference of combanation\n",
    "For example, if there is a observation that is at the same hour, one day before target date, but 1000km away.\n",
    "Using summation model it will still result in large weight because of hour and date. On the other hand, using multiplication the small weight of distance will keep the overall weight down. We will say that the multiplication model is more ideal because we need to consider all three kernels when doing prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b3f91",
   "metadata": {},
   "source": [
    "#### Ans3.\n",
    "For the updated code, we use RandomForest and Decision Tree within the loop to train for each hour.\n",
    "\n",
    "The first model we use RandomForeset(RF), since last time when we use LinearRegression(LR), the result of LR was not ideal and requires higer run time in order to use better parameters to avoid large weight and wrong prediction result.  \n",
    "\n",
    "The Second model we use is Decision Tree regression(DT) since it is usable to capture non-linear relationship between variables (same for RF).\n",
    "\n",
    "In terms of result, the RF prediction has higer temperature druing noon and decresase over night, which is as expected. However, since the time constraint we need to tune the parameter to lower tree count, shallow depth, smallerBins. These parameters setting makes the result of 06:00:00 being very low. Using better parameters can improve this, but will take much longer to run.  \n",
    "  \n",
    "For Decision Tree(DT), the time of higher and lower temperature are also as expected. We also use shallow depth here to improve speed.  \n",
    "  \n",
    "Overall, we will say that both RF and DT prediction are similar but not perfect. Also, the way we handle data (by giving reference starting point for location and date) have make our MLlib model result different to Gaussian kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f8cf3-ceb4-4ae0-a239-9d80e7cddaec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
