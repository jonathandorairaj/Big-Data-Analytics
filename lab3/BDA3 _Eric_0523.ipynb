{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72d34265",
   "metadata": {},
   "source": [
    "## Kernel Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d2bfcf",
   "metadata": {},
   "source": [
    "from __future__ import division\n",
    "from math import radians, cos, sin, asin, sqrt, exp\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark.broadcast import Broadcast\n",
    "import numpy as np\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "def day_difference(day1, day2):\n",
    "    diff = abs(datetime.strptime(str(day1), \"%Y-%m-%d\") - datetime.strptime(str(day2), \"%Y-%m-%d\"))\n",
    "    no_days = diff.days\n",
    "    return no_days\n",
    "\n",
    "def hour_diff(time1, time2):\n",
    "    diff = abs(datetime.strptime(time1, \"%H:%M:%S\") - datetime.strptime(time2, \"%H:%M:%S\"))\n",
    "    diff = (diff.total_seconds()) / 3600\n",
    "    return diff\n",
    "\n",
    "def gaussian_kernel(u, h):\n",
    "    return np.exp(-u**2 / (2 * h**2))\n",
    "\n",
    "def sum_kernel(distance_kernel, day_kernel, time_kernel):\n",
    "    res = distance_kernel + day_kernel + time_kernel\n",
    "    return res\n",
    "\n",
    "def product_kernel(distance_kernel, day_kernel, time_kernel):\n",
    "    res = distance_kernel * day_kernel * time_kernel\n",
    "    return res\n",
    "\n",
    "# Value to predict\n",
    "target_latitude = 58.68681\n",
    "target_longitude = 15.92183\n",
    "target_date = '2014-05-17'\n",
    "hour_list= [\"00:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\", \"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]\n",
    "\n",
    "#target_date = '1980-05-17'\n",
    "#hour_list= [\"00:00:00\", \"22:00:00\"]\n",
    "\n",
    "# Hyperparameters: widths for day, distance, and time\n",
    "h_dist = 100\n",
    "h_day = 30\n",
    "h_time = 12\n",
    "\n",
    "# Create SparkContext\n",
    "sc = SparkContext(appName=\"Lab 3 ML\")\n",
    "\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "station_file = sc.textFile(\"BDA/input/stations.csv\")\n",
    "\n",
    "temperature_data = temperature_file.map(lambda x: x.split(';'))\n",
    "stations_data = station_file.map(lambda x: x.split(';'))\n",
    "\n",
    "target_date_strip = datetime.strptime(target_date, '%Y-%m-%d')\n",
    "prev_temp = temperature_data.filter(lambda x: datetime.strptime(x[1], '%Y-%m-%d') < target_date_strip)\n",
    "\n",
    "# Calculate station_distkernel and broadcast it\n",
    "station_distkernel = stations_data.map(lambda x: (x[0], gaussian_kernel(haversine(target_longitude, target_latitude, float(x[4]), float(x[3])), h_dist))).collectAsMap()\n",
    "broadcast_station_distkernel = sc.broadcast(station_distkernel)\n",
    "\n",
    "temperature_data_datekernel = prev_temp.map(lambda x: (x[0], x[1], x[2], x[3], gaussian_kernel(day_difference(target_date, x[1]), h_day))).cache()\n",
    "#print(\"You are here 0\")\n",
    "#print(temperature_data_datekernel.take(1))\n",
    "\n",
    "predictions = {}\n",
    "for hour in hour_list:\n",
    "    temp_kernels = temperature_data_datekernel.map(lambda x: (float(x[3]),\n",
    "                                                broadcast_station_distkernel.value[x[0]],\n",
    "                                                x[4],\n",
    "                                                gaussian_kernel(hour_diff(hour, x[2]), h_time)))\n",
    "    #temp,sumkernel, prodkernel                                \n",
    "    #temp_both_kernels = temp_kernels.map(lambda x: (x[0], sum_kernel(x[1], x[2], x[3]), product_kernel(x[1], x[2], x[3])))\n",
    "    #print(\"You are here 1\")\n",
    "    #print(temp_both_kernels.take(1))\n",
    "    sum_kernel, sum_kernel*temp, prod_kernel, prod_kernel*temp \n",
    "    cal_kerneltemp = temp_both_kernels.map(lambda x :(x[1],x[0]*x[1],x[2],x[0]*x[2]))\n",
    "\n",
    "\t\n",
    "    #print(\"You are here 2\")\n",
    "    #print(cal_kerneltemp.take(1))\n",
    "    sum_kerneltemp = cal_kerneltemp.reduce(lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2], x[3] + y[3]))\n",
    "    #print(\"You are here 3\")\n",
    "    #print(sum_kerneltemp)\n",
    "    predictions[hour]=(sum_kerneltemp[1]/sum_kerneltemp[0],sum_kerneltemp[3]/sum_kerneltemp[2])\n",
    "\n",
    "# Convert predictions dictionary to RDD\n",
    "predictions_rdd = sc.parallelize(list(predictions.items()))\n",
    "# Coalesce and sort the RDD\n",
    "predictions_rdd = predictions_rdd.coalesce(1)\n",
    "predictions_rdd = predictions_rdd.sortByKey()\n",
    "# Save the RDD as text files\n",
    "predictions_rdd.saveAsTextFile(\"BDA/output/predictions\")                                                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc27eb3",
   "metadata": {},
   "source": [
    "## Time(Hour),Prediction using summation kernel, Prediction using Prodoct kernel\n",
    "('00:00:00', (4.6183467454689477, 6.2916698934306972))  \n",
    "('04:00:00', (4.7065925124492409, 6.5930895159243779))  \n",
    "('06:00:00', (4.7595837225876654, 6.7317721621128745))  \n",
    "('08:00:00', (4.8147176456957137, 6.8593316278114189))  \n",
    "('10:00:00', (4.8698045237472147, 6.9736545639289158))  \n",
    "('12:00:00', (4.9231523044830698, 7.0729286567357832))  \n",
    "('14:00:00', (4.973566225831247, 7.1557335681263039))  \n",
    "('16:00:00', (5.0203553843682771, 7.2211071751239935))  \n",
    "('18:00:00', (5.0633437042742591, 7.2685807021593085))  \n",
    "('20:00:00', (5.1028828756959461, 7.2981802141488545))  \n",
    "('22:00:00', (5.1398646881935637, 7.3103960959870617))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bf9cf4",
   "metadata": {},
   "source": [
    "## MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d0b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from datetime import datetime\n",
    "from math import radians, cos, sin, asin, sqrt, exp\n",
    "from pyspark.broadcast import Broadcast\n",
    "\n",
    "# Function to calculate haversine distance\n",
    "def haversine(lon1, lat1, lon2=16.321998712, lat2=62.38583179): # GEOGRAPHICAL CENTER OF SWEDEN\n",
    "    # convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return km\n",
    "\n",
    "# Function to calculate the difference in days between two dates\n",
    "def day_diff(day1, day2=\"1950-01-01\"):\n",
    "    diff = abs(datetime.strptime(str(day1), \"%Y-%m-%d\") - datetime.strptime(str(day2), \"%Y-%m-%d\"))\n",
    "    no_days = diff.days\n",
    "    return no_days\n",
    "\n",
    "# Function to calculate the difference in hours between two times\n",
    "def hour_diff(time1, time2=\"00:00:00\"):\n",
    "    diff = abs(datetime.strptime(time1, \"%H:%M:%S\") - datetime.strptime(time2, \"%H:%M:%S\"))\n",
    "    diff = (diff.total_seconds()) / 3600\n",
    "    return diff\n",
    "\n",
    "\n",
    "sc = SparkContext(appName=\"Lab 3 ML\")\n",
    "target_date = '2014-12-31'\n",
    "target_latitude = 58.68681\n",
    "target_longitude = 15.92183\n",
    "\n",
    "target_distance = haversine(lon1=target_longitude, lat1=target_latitude) \n",
    "target_date_diff = day_diff(day1=target_date)\n",
    "\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "#temperature_file = temperature_file.sample(withReplacement=False, fraction=0.1)\n",
    "temperature_data = temperature_file.map(lambda x: x.split(';'))\n",
    "print(temperature_data.take(1))\n",
    "print(\"You are here0\")\n",
    "\n",
    "target_date_strip = datetime.strptime(target_date, '%Y-%m-%d')\n",
    "prev_temp = temperature_data.filter(lambda x: datetime.strptime(x[1], '%Y-%m-%d') < target_date_strip)\n",
    "print(prev_temp.take(1))\n",
    "print(\"You are here1\")\n",
    "\n",
    "station_file = sc.textFile(\"BDA/input/stations.csv\")\n",
    "stations_data = station_file.map(lambda x: x.split(';'))\n",
    "\n",
    "stations_distance = stations_data.map(lambda x: (x[0], haversine(lat1=float(x[3]), lon1=float(x[4]))))\n",
    "\n",
    "print(\"You are here2\")\n",
    "stations_distance = stations_distance.collectAsMap()\n",
    "broadcast_stations_distance = sc.broadcast(stations_distance)\n",
    "\n",
    "# for training, the plan is to use the distance from lon=lat=0 as the original point\n",
    "# how many dates have passed since 1950/01/01\n",
    "# and the hour difference from 00:00:00\n",
    "# training features are daydiff, hourdiff, distance\n",
    "training_temp = prev_temp.map(lambda x: (\n",
    "    float(x[3]), day_diff(x[1]), hour_diff(x[2]), broadcast_stations_distance.value[x[0]]))\n",
    "\n",
    "\n",
    "print(\"You are here3\")\n",
    "####standardized####\n",
    "# Note, since features are standardized, I think we will have to standardize the target as well,\n",
    "# or the prediction will be super bad\n",
    "# Or we can try not standardized it, but I did not get it to work at this point\n",
    "features = training_temp.map(lambda x: x[1:])\n",
    "standardizer = StandardScaler()\n",
    "model = standardizer.fit(features)\n",
    "features_transform = model.transform(features)\n",
    "label = training_temp.map(lambda x: x[0])\n",
    "standardized_data = label.zip(features_transform)\n",
    "labeledpoint = standardized_data.map(lambda x: LabeledPoint(x[0], [x[1]]))\n",
    "print(labeledpoint.take(10))\n",
    "print(\"above is the labeledpoint\")\n",
    "#####################\n",
    "\n",
    "linearModel = LinearRegressionWithSGD.train(labeledpoint, regType='l2', step=0.25, iterations = 50)\n",
    "dt_model = DecisionTree.trainRegressor(labeledpoint, categoricalFeaturesInfo={})\n",
    "\n",
    "######################\n",
    "# Create a RDD for your target features\n",
    "hour_list = [\"00:00:00\", \"22:00:00\", \"20:00:00\", \"18:00:00\", \"16:00:00\", \"14:00:00\",\n",
    "             \"12:00:00\", \"10:00:00\", \"08:00:00\", \"06:00:00\", \"04:00:00\"]\n",
    "\n",
    "target_features = []\n",
    "for hour in hour_list:\n",
    "    target_hour = hour_diff(time1=hour)\n",
    "    target_feature = Vectors.dense([float(target_date_diff), target_hour, target_distance])\n",
    "    target_features.append(target_feature)\n",
    "target_features_rdd = sc.parallelize(target_features)\n",
    "\n",
    "print(\"----target_features_rdd-----\")\n",
    "print(target_features_rdd.take(10))\n",
    "print(\"----target_features_rdd-----\")\n",
    "\n",
    "target_features_transform = model.transform(target_features_rdd)\n",
    "standardized_target_features = target_features_transform.collect()\n",
    "#print(\"----standardized_target_features-----\")\n",
    "#print(standardized_target_features)\n",
    "#print(\"----standardized_target_features-----\")\n",
    "\n",
    "\n",
    "print(\"------Linear-------\")\n",
    "lr_predictions = [linearModel.predict(feature) for feature in standardized_target_features]\n",
    "print(lr_predictions)\n",
    "print(\"------Linear-------\")\n",
    "\n",
    "print(\"------DT-------\")\n",
    "dt_predictions = [dt_model.predict(feature) for feature in standardized_target_features]\n",
    "print(dt_predictions)\n",
    "print(\"------DT-------\")\n",
    "\n",
    "\n",
    "# save the predictions to a text file\n",
    "lr_predictions = sc.parallelize(lr_predictions)\n",
    "lr_predictions = lr_predictions.coalesce(1)\n",
    "lr_predictions.saveAsTextFile(\"BDA/output/lr_prediction\")\n",
    "# save the predictions to a text file\n",
    "dt_predictions = sc.parallelize(dt_predictions)\n",
    "dt_predictions = dt_predictions.coalesce(1)\n",
    "dt_predictions.saveAsTextFile(\"BDA/output/dt_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d7121",
   "metadata": {},
   "source": [
    "Output of linear regression    \n",
    "3.34001769622  \n",
    "6.26347240094  \n",
    "5.99770379142  \n",
    "5.7319351819  \n",
    "5.46616657238  \n",
    "5.20039796286  \n",
    "4.93462935334  \n",
    "4.66886074382  \n",
    "4.4030921343  \n",
    "4.13732352478  \n",
    "3.87155491526  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bfb287",
   "metadata": {},
   "source": [
    "Output of Decision Tree  \n",
    "3.22156055458  \n",
    "3.87637925605  \n",
    "3.87637925605  \n",
    "5.22146646785  \n",
    "6.63698189402  \n",
    "6.63698189402  \n",
    "6.63698189402  \n",
    "6.63698189402  \n",
    "4.98940174481  \n",
    "3.22156055458  \n",
    "3.22156055458  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
