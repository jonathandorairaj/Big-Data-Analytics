{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ac09cb",
   "metadata": {},
   "source": [
    "## Assignments\n",
    "### Q1 What are thelowest and highest temperatures measured each year for the period 1950-2014.\n",
    "Provide the lists sorted in the descending order with respect to the maximum temperature. In\n",
    "this exercise you will use the temperature-readings.csv file.\n",
    "The output should at least contain the following information (You can also include a Station\n",
    "column so that you may find multiple stations that record the highest (lowest)\n",
    "temperature.):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24de305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName = \"exercise 1\")\n",
    "# This path is to the file on hdfs\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "lines = temperature_file.map(lambda line: line.split(\";\"))\n",
    "\n",
    "# (key, value) = (year,temperature)\n",
    "year_temperature = lines.map(lambda x: (x[1][0:4], (x[0],float(x[3]))))\n",
    "\n",
    "#filter\n",
    "year_temperature = year_temperature.filter(lambda x: int(x[0])>=1950 or int(x[0])<=2014)\n",
    "\n",
    "#Get max\n",
    "max_temperatures = year_temperature.reduceByKey(lambda a, b: (a[0], a[1]) if a[1] > b[1] else (b[0],b[1]))\n",
    "#max_temperatures = max_temperatures.sortBy(ascending = False, keyfunc=lambda k: k[1])\n",
    "\n",
    "\n",
    "#get min \n",
    "min_temperatures = year_temperature.reduceByKey(lambda a, b: (a[0], a[1]) if a[1] < b[1] else (b[0],b[1]))\n",
    "#min_temperatures = min_temperatures.sortBy(ascending = False, keyfunc=lambda k: k[1])\n",
    "\n",
    "joineddf = max_temperatures.join(min_temperatures)\n",
    "joineddf = joineddf.sortBy(ascending = False, keyfunc = lambda k : k[1][0][1])\n",
    "\n",
    "#print(max_temperatures.collect())\n",
    "\n",
    "# Following code will save the result into /user/ACCOUNT_NAME/BDA/output folder\n",
    "joineddf.saveAsTextFile(\"BDA/output\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27604f7f",
   "metadata": {},
   "source": [
    "* Output of Q1 : \n",
    "\n",
    "#### Year    Station, max_temp   Station,min_temp\n",
    "(u'1975', ((u'86200', 36.1), (u'157860', -37.0)))  \n",
    "(u'1992', ((u'63600', 35.4), (u'179960', -36.1)))  \n",
    "(u'1994', ((u'117160', 34.7), (u'179960', -40.5)))  \n",
    "(u'2014', ((u'96560', 34.4), (u'192840', -42.5)))  \n",
    "(u'2010', ((u'75250', 34.4), (u'191910', -41.7)))  \n",
    "(u'1947', ((u'53770', 34.3), (u'139570', -32.0)))  \n",
    "(u'1989', ((u'63050', 33.9), (u'166870', -38.2)))  \n",
    "(u'1982', ((u'94050', 33.8), (u'113410', -42.2)))  \n",
    "(u'1968', ((u'137100', 33.7), (u'179950', -42.0)))  \n",
    "(u'1966', ((u'151640', 33.5), (u'179950', -49.4)))  \n",
    "(u'1945', ((u'85600', 33.4), (u'139570', -26.3)))  \n",
    "(u'2002', ((u'78290', 33.3), (u'169860', -42.2)))  \n",
    "(u'1983', ((u'98210', 33.3), (u'191900', -38.2)))  \n",
    "(u'1986', ((u'76470', 33.2), (u'167860', -44.2)))  \n",
    "(u'1970', ((u'103080', 33.2), (u'179950', -39.6)))  \n",
    "(u'2015', ((u'125170', 33.1), (u'179960', -39.9)))  \n",
    "(u'1956', ((u'145340', 33.0), (u'160790', -45.0)))  \n",
    "(u'2000', ((u'62400', 33.0), (u'169860', -37.6)))  \n",
    "(u'1959', ((u'65160', 32.8), (u'159970', -43.6)))  \n",
    "(u'1991', ((u'137040', 32.7), (u'179960', -39.3)))  \n",
    "(u'2006', ((u'75240', 32.7), (u'169860', -40.6)))  \n",
    "(u'1988', ((u'102540', 32.6), (u'170790', -39.9)))  \n",
    "(u'2011', ((u'172770', 32.5), (u'179960', -42.0)))  \n",
    "(u'1999', ((u'98210', 32.4), (u'192830', -49.0)))  \n",
    "(u'1948', ((u'53770', 32.4), (u'139570', -30.0)))  \n",
    "(u'1955', ((u'97260', 32.2), (u'160790', -41.2)))  \n",
    "(u'2007', ((u'86420', 32.2), (u'169860', -40.7)))  \n",
    "(u'2008', ((u'102390', 32.2), (u'179960', -39.3)))  \n",
    "(u'2003', ((u'136420', 32.2), (u'179960', -41.5)))    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a757b534",
   "metadata": {},
   "source": [
    "### Q2 Count the number of readings for each month in the period of 1950-2014 which are higher than 10 degrees.\n",
    "Repeat the exercise,this time taking only distinct readings from each station.\n",
    "That is, if a station reported a reading above 10 degrees in some month, then it appears only\n",
    "once in the count for that month.\n",
    "In this exercise you will use the temperature-readings.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658842fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName = \"exercise 2\")\n",
    "# This path is to the file on hdfs\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "lines = temperature_file.map(lambda line: line.split(\";\"))\n",
    "\n",
    "# (key, value) = ((Year, month), temp)\n",
    "year_month_temperature = lines.map(lambda x: ((x[1][0:4],x[1][5:7],x[0]),float(x[3])))\n",
    "\n",
    "\n",
    "#filter\n",
    "year_month_temperature = year_month_temperature.filter(lambda x: int(x[0][0]) >= 1950 and int(x[0][0]) <=2014 and x[1] > 10)\n",
    "#count = year_month_temperature.map(lambda x: (x[0], 1))\n",
    "count = year_month_temperature.map(lambda x: ((x[0][0],x[0][1]), 1))\n",
    "count = count.reduceByKey(lambda a, b: a + b)\n",
    "count = count.coalesce(1)\n",
    "count_sort = count.sortByKey().sortByKey(1)\n",
    "count_sort.saveAsTextFile(\"BDA/output/countsort\")\n",
    "########################\n",
    "\n",
    "\n",
    "\n",
    "count_distinct = year_month_temperature.map(lambda x: (x[0],1)).distinct()\n",
    "count_distinct = count_distinct.map(lambda x: ((x[0][0],x[0][1]), 1))\n",
    "count_distinct = count_distinct.reduceByKey(lambda a, b: a + b)\n",
    "count_distinct = count_distinct.coalesce(1)\n",
    "count_distinct_sort = count_distinct.sortByKey().sortByKey(1)\n",
    "count_distinct_sort.saveAsTextFile(\"BDA/output/count_distinct_sort\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a698175",
   "metadata": {},
   "source": [
    "* Output of countsort (the number of readings for each month)  \n",
    "\n",
    "#### Year, month, count (sorted by date)\n",
    "\n",
    "((u'1950', u'03'), 81)  \n",
    "((u'1950', u'04'), 352)  \n",
    "((u'1950', u'05'), 2802)  \n",
    "((u'1950', u'06'), 4886)  \n",
    "((u'1950', u'07'), 5811)  \n",
    "((u'1950', u'08'), 5954)  \n",
    "((u'1950', u'09'), 3612)  \n",
    "((u'1950', u'10'), 1248)  \n",
    "((u'1950', u'11'), 2)  \n",
    "((u'1950', u'12'), 1)  \n",
    "((u'1951', u'02'), 1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d385b",
   "metadata": {},
   "source": [
    "* Output of count_distinct_sort(the number of distinct readings for each month)  \n",
    "\n",
    "#### Year, month, count (sorted by date)\n",
    "\n",
    "((u'1950', u'03'), 26)  \n",
    "((u'1950', u'04'), 36)  \n",
    "((u'1950', u'05'), 46)  \n",
    "((u'1950', u'06'), 47)  \n",
    "((u'1950', u'07'), 49)  \n",
    "((u'1950', u'08'), 49)  \n",
    "((u'1950', u'09'), 50)  \n",
    "((u'1950', u'10'), 46)  \n",
    "((u'1950', u'11'), 2)  \n",
    "((u'1950', u'12'), 1)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7326f9",
   "metadata": {},
   "source": [
    "### Q3.Find the average monthly temperature for each available station in Sweden. \n",
    "Your result should include average temperature for each station for each month in the period of 1960-\n",
    "2014. Bear in mind that not every station has the readings for each month in this timeframe. \n",
    "In this exercise you will use the temperature-readings.csv file.\n",
    "\n",
    "The output should contain the following information:\n",
    "\n",
    "Year, month, station number, average monthly temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f50f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName = 'exercise 3')\n",
    "temperature_file = sc.textFile('BDA/input/temperature-readings.csv')\n",
    "\n",
    "lines = temperature_file.map(lambda line: line.split(';'))\n",
    "\n",
    "year_month_date_station_temp = lines.map(lambda x: ( (x[1][0:4],x[1][5:7],x[1][8:],x[0]) , (float(x[3])) ) )\n",
    "\n",
    "year_month_date_station_temp =year_month_date_station_temp.filter(lambda x : int(x[0][0])>=1960 and int(x[0][0])<=2014)\n",
    "\n",
    "min_max_temperatures = year_month_date_station_temp.groupByKey()\n",
    "min_max_temperatures = min_max_temperatures.mapValues(lambda x: (min(x),max(x)))\n",
    "\n",
    "# calculating daily average\n",
    "avg_temperature = min_max_temperatures.map(lambda x: ((x[0][0], x[0][1],x[0][3]), (x[1][0] + x[1][1]) / 2))\n",
    "\n",
    "#add count column \n",
    "avg_temperature = avg_temperature.mapValues(lambda x : (x,1))\n",
    "\n",
    "avg_monthly_temperature = avg_temperature.reduceByKey(lambda a,b : (a[0] + b[0],a[1] + b[1]) )\n",
    "avg_monthly_temperature = avg_monthly_temperature.mapValues(lambda x : (x[0]/x[1]))\n",
    "\n",
    "\n",
    "avg_monthly_temperature.saveAsTextFile('BDA/output/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7496f72b",
   "metadata": {},
   "source": [
    "* Output of Q3 \n",
    "\n",
    "#### Year, month, station number, average monthly temperature\n",
    "((u'1989', u'06', u'92400'), 14.686666666666666)  \n",
    "((u'1982', u'09', u'107530'), 11.171666666666669)  \n",
    "((u'2002', u'11', u'136360'), -5.861666666666666)  \n",
    "((u'1967', u'08', u'98170'), 15.408064516129032)  \n",
    "((u'2002', u'08', u'181900'), 15.598387096774195)  \n",
    "((u'1981', u'11', u'63440'), 3.086666666666667)  \n",
    "((u'1996', u'08', u'96190'), 17.1)  \n",
    "((u'1994', u'06', u'71180'), 13.036666666666669)  \n",
    "((u'2010', u'10', u'64130'), 5.974193548387096)  \n",
    "((u'1995', u'06', u'62400'), 16.00166666666667)  \n",
    "((u'1972', u'10', u'64130'), 7.666129032258065)  \n",
    "((u'1985', u'02', u'81130'), -7.678571428571428)  \n",
    "((u'1977', u'10', u'191900'), -2.9322580645161294)  \n",
    "((u'1988', u'04', u'86330'), 4.626666666666666)  \n",
    "((u'1989', u'04', u'180940'), -0.43333333333333346)  \n",
    "((u'1980', u'02', u'123250'), -14.946551724137931)  \n",
    "((u'1964', u'04', u'53640'), 7.694999999999999)  \n",
    "((u'1984', u'05', u'106100'), 10.933870967741937)  \n",
    "((u'2002', u'09', u'178860'), 6.408333333333333)  \n",
    "((u'1977', u'08', u'182930'), 10.193548387096774)  \n",
    "((u'1983', u'02', u'78240'), -2.2660714285714287)  \n",
    "((u'1967', u'10', u'162880'), 3.229032258064516)  \n",
    "((u'1990', u'02', u'89240'), 3.692857142857143)  \n",
    "((u'1966', u'04', u'137110'), 0.23500000000000004)  \n",
    "((u'1990', u'07', u'52360'), 16.559677419354838)  \n",
    "((u'2000', u'05', u'73470'), 12.243548387096775)  \n",
    "((u'1979', u'01', u'123480'), -19.470967741935485)  \n",
    "((u'1985', u'08', u'95160'), 15.506451612903227)  \n",
    "((u'1970', u'01', u'83340'), -7.161290322580644)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a204c",
   "metadata": {},
   "source": [
    "### Q4 Provide a list of stations with their associated maximum measured temperatures and precipitation\n",
    "maximum measured daily precipitation. Show only those stations where the maximum\n",
    "temperature is between 25 and 30 degrees and maximum daily precipitation is between 100\n",
    "mm and 200mm.\n",
    "In this exercise you will use the temperature-readings.csv and precipitation-readings.csv\n",
    "files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dacc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName = \"exercise 4\")\n",
    "# This path is to the file on hdfs\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "precipitation_file = sc.textFile(\"BDA/input/precipitation-readings.csv\")\n",
    "\n",
    "temperature_lines = temperature_file.map(lambda line: line.split(\";\"))\n",
    "precipitation_file = precipitation_file.map(lambda line: line.split(\";\"))\n",
    "\n",
    "get_temperature = temperature_lines.map(lambda x: (x[0],float(x[3])))\n",
    "get_percipitation = precipitation_file.map(lambda x: (x[0],float(x[3])))\n",
    "\n",
    "max_temp = get_temperature.reduceByKey(max)\n",
    "filter_temp = max_temp.filter(lambda x : x[1]>25 and x[1]<30)\n",
    "\n",
    "\n",
    "max_perc = get_percipitation.reduceByKey(max)\n",
    "filter_perc = max_perc.filter(lambda x : x[1]>1000 and x[1]<200)\n",
    "\n",
    "\n",
    "\n",
    "join_output= filter_temp.join(filter_perc)\n",
    "join_output = join_output.coalesce(1)\n",
    "join_output_sort = join_output.sortByKey()\n",
    "join_output_sort.saveAsTextFile(\"BDA/output/temp_perce_sort\")\n",
    "\n",
    "### Below are for testing since no output\n",
    "#filter_perc = max_perc.filter(lambda x : x[1]>0 and x[1]<15) \n",
    "#filter_perc = max_perc.filter(lambda x : x[1]>100) \n",
    "#filter_temp.saveAsTextFile(\"BDA/output/temp_test\")\n",
    "#max_perc.saveAsTextFile(\"BDA/output/maxperc_test\")\n",
    "#filter_perc.saveAsTextFile(\"BDA/output/filterperc_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba4b979",
   "metadata": {},
   "source": [
    "#### The output for temp_perce_sort is empty, since no data meet the criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b8ef1",
   "metadata": {},
   "source": [
    "### Q5. Calculate the average monthly precipitation for the Östergotland region \n",
    "(list of stations is\n",
    "provided in the separate file) for the period 1993-2016. In orderto dothis, you will first need to calculate the total monthly precipitation for each station before calculating the monthly\n",
    "average (by averaging over stations).\n",
    "In this exercise you will use the precipitation-readings.csv and stations-Ostergotland.csv\n",
    "files. HINT (not for the SparkSQL lab): Avoid using joins here! stations-Ostergotland.csv is\n",
    "small and if distributed will cause a number of unnecessary shuffles when joined with\n",
    "precipitationRDD. If you distribute precipitation-readings.csv then either repartition your\n",
    "stations RDD to 1 partition or make use of the collect function to acquire a python list and\n",
    "broadcast function to broadcast the list to all nodes.\n",
    "The output should contain the following information:  \n",
    "\n",
    "Year, month, average monthly precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName = 'exercise 3')\n",
    "precipitaion_file = sc.textFile('BDA/input/precipitation-readings.csv')\n",
    "stations_file = sc.textFile('BDA/input/stations-Ostergotland.csv')\n",
    "\n",
    "lines = precipitaion_file.map(lambda line: line.split(';'))\n",
    "stations = stations_file.map(lambda line: line.split(';'))\n",
    "\n",
    "# extracting only the station numbers, then collecting and broadcasting to make available to all nodes to filter later\n",
    "stations = stations.map(lambda x: x[0])\n",
    "stations = stations.collect()\n",
    "stations = sc.broadcast(stations).value\n",
    "\n",
    "year_month_station_precip = lines.map(lambda x: ( (x[1][0:4],x[1][5:7],x[0]) , (float(x[3])) ) )\n",
    "#filter for years\n",
    "year_month_station_precip =year_month_station_precip.filter(lambda x : int(x[0][0])>=1993 and int(x[0][0])<=2016)\n",
    "\n",
    "#filter for stations in Ostergotland\n",
    "year_month_station_precip = year_month_station_precip.filter(lambda x : x[0][2] in stations)\n",
    "\n",
    "#summing up to get total precipitation per month,station and year\n",
    "year_month_station_precip = year_month_station_precip.reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "# remap to add count column in value\n",
    "monthly_precipitation = year_month_station_precip.map(lambda x: ((x[0][0],x[0][1]),(x[1],1)))\n",
    "\n",
    "#summing up\n",
    "monthly_precipitation = monthly_precipitation.reduceByKey(lambda a,b: (a[0] + b[0],a[1] + b[1]))\n",
    "\n",
    "#obtaining average\n",
    "avg_monthly_precipitaion= monthly_precipitation.mapValues(lambda x : (x[0]/x[1]))\n",
    "\n",
    "\n",
    "avg_monthly_precipitaion.saveAsTextFile('BDA/output/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e3186",
   "metadata": {},
   "source": [
    "* Output of Q5 \n",
    "### Year, month, average monthly precipitation\n",
    "\n",
    "((u'2012', u'09'), 72.75)  \n",
    "((u'1995', u'05'), 26.00000000000002)  \n",
    "((u'1996', u'12'), 39.55000000000003)  \n",
    "((u'2011', u'08'), 86.26666666666667)  \n",
    "((u'2007', u'04'), 21.249999999999996)  \n",
    "((u'2007', u'06'), 108.94999999999999)  \n",
    "((u'1993', u'04'), 0.0)  \n",
    "((u'2011', u'10'), 43.75)  \n",
    "((u'2014', u'10'), 72.13749999999999)  \n",
    "((u'1996', u'09'), 57.46666666666667)  \n",
    "((u'1995', u'07'), 43.6)  \n",
    "((u'2002', u'05'), 72.13333333333334)  \n",
    "((u'2010', u'04'), 23.78333333333333)  \n",
    "((u'1999', u'01'), 61.933333333333394)  \n",
    "((u'2013', u'11'), 46.37500000000002)  \n",
    "((u'2016', u'05'), 29.250000000000004)  \n",
    "((u'1999', u'10'), 18.549999999999997) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f3e29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
