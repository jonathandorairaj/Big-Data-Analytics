{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0f0c8c-78eb-4337-89eb-eb0f23b84125",
   "metadata": {},
   "source": [
    "## Assignments\n",
    "### Q1 What are thelowest and highest temperatures measured each year for the period 1950-2014.\n",
    "Provide the lists sorted in the descending order with respect to the maximum temperature. In\n",
    "this exercise you will use the temperature-readings.csv file.\n",
    "The output should at least contain the following information (You can also include a Station\n",
    "column so that you may find multiple stations that record the highest (lowest)\n",
    "temperature.):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2764eb78-d8f4-41bf-80f9-5c2885da2157",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[0;32m      3\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(appName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexercise 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This path is to the file on hdfs\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName = \"exercise 1\")\n",
    "# This path is to the file on hdfs\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "lines = temperature_file.map(lambda line: line.split(\";\"))\n",
    "\n",
    "# (key, value) = (year,(station,temperature))\n",
    "year_temperature = lines.map(lambda x: (x[1][0:4], (x[0],float(x[3]))))\n",
    "\n",
    "#filter\n",
    "year_temperature = year_temperature.filter(lambda x: int(x[0])>=1950 and int(x[0])<=2014)\n",
    "\n",
    "#Get max\n",
    "max_temperatures = year_temperature.reduceByKey(lambda a,b: (a[0], max(a[1], b[1])))\n",
    "max_temperatures = max_temperatures.sortBy(lambda x: x[1][1], ascending=False)\n",
    "\n",
    "#Get max\n",
    "min_temperatures = year_temperature.reduceByKey(lambda a,b: (a[0], min(a[1], b[1])))\n",
    "min_temperatures = min_temperatures.sortBy(lambda x: x[1][1], ascending=False)\n",
    "\n",
    "\n",
    "max_temperatures_combine = max_temperatures.coalesce(1)\n",
    "max_temperatures_combine = max_temperatures_combine.sortBy(lambda x: x[1][1], ascending=False)\n",
    "max_temperatures_combine.saveAsTextFile(\"BDA/output/l1max\")\n",
    "\n",
    "min_temperatures_combine = min_temperatures.coalesce(1)\n",
    "min_temperatures_combine = min_temperatures_combine.sortBy(lambda x: x[1][1], ascending=False)\n",
    "min_temperatures_combine.saveAsTextFile(\"BDA/output/l1min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade982a0-81f0-4fb0-905f-cab62661d499",
   "metadata": {},
   "source": [
    "* Output of l1max\n",
    "(u'1975', (u'102190', 36.1))  \n",
    "(u'1992', (u'112080', 35.4))  \n",
    "(u'1994', (u'123250', 34.7))  \n",
    "(u'2014', (u'123340', 34.4))  \n",
    "(u'2010', (u'123340', 34.4))  \n",
    "(u'1989', (u'112080', 33.9))  \n",
    "(u'1982', (u'133260', 33.8))  \n",
    "(u'1968', (u'133470', 33.7))  \n",
    "(u'1966', (u'102190', 33.5))  \n",
    "(u'2002', (u'123250', 33.3))  \n",
    "(u'1983', (u'123250', 33.3))  \n",
    "(u'1986', (u'123250', 33.2))  \n",
    "(u'1970', (u'112080', 33.2))  \n",
    "(u'2000', (u'102190', 33.0))  \n",
    "(u'1956', (u'108640', 33.0))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb92900-1767-4599-82cf-b7adf647a7f6",
   "metadata": {},
   "source": [
    "* Output of l1min\n",
    "(u'1990', (u'133260', -35.0))  \n",
    "(u'1952', (u'108640', -35.5))  \n",
    "(u'1974', (u'112080', -35.6))  \n",
    "(u'1954', (u'134110', -36.0))  \n",
    "(u'1992', (u'112080', -36.1))  \n",
    "(u'1975', (u'123480', -37.0))  \n",
    "(u'1972', (u'123480', -37.5))  \n",
    "(u'1995', (u'102210', -37.6))  \n",
    "(u'2000', (u'123250', -37.6))  \n",
    "(u'1957', (u'123480', -37.8))  \n",
    "(u'1983', (u'133260', -38.2))  \n",
    "(u'1989', (u'112080', -38.2))  \n",
    "(u'1953', (u'124020', -38.4))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21277992-c7d8-415c-8ea8-ef137db62c8e",
   "metadata": {},
   "source": [
    "### Q2 Count the number of readings for each month in the period of 1950-2014 which are higher than 10 degrees.\n",
    "Repeat the exercise,this time taking only distinct readings from each station.\n",
    "That is, if a station reported a reading above 10 degrees in some month, then it appears only\n",
    "once in the count for that month.\n",
    "In this exercise you will use the temperature-readings.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5479cd-351f-40b4-a323-d549993ea164",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[0;32m      3\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(appName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexercise 2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This path is to the file on hdfs\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName = \"exercise 2\")\n",
    "# This path is to the file on hdfs\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "lines = temperature_file.map(lambda line: line.split(\";\"))\n",
    "\n",
    "# (key, value) = ((Year, month), temp)\n",
    "year_month_temperature = lines.map(lambda x: ((x[1][0:4],x[1][5:7],x[0]),float(x[3])))\n",
    "\n",
    "\n",
    "#filter\n",
    "year_month_temperature = year_month_temperature.filter(lambda x: int(x[0][0]) >= 1950 and int(x[0][0]) <=2014 and x[1] > 10)\n",
    "#count = year_month_temperature.map(lambda x: (x[0], 1))\n",
    "count = year_month_temperature.map(lambda x: ((x[0][0],x[0][1]), 1))\n",
    "count = count.reduceByKey(lambda a, b: a + b)\n",
    "count = count.coalesce(1)\n",
    "count_sort = count.sortByKey().sortByKey(1)\n",
    "count_sort.saveAsTextFile(\"BDA/output/countsort\")\n",
    "########################\n",
    "\n",
    "\n",
    "\n",
    "count_distinct = year_month_temperature.map(lambda x: (x[0],1)).distinct()\n",
    "count_distinct = count_distinct.map(lambda x: ((x[0][0],x[0][1]), 1))\n",
    "count_distinct = count_distinct.reduceByKey(lambda a, b: a + b)\n",
    "count_distinct = count_distinct.coalesce(1)\n",
    "count_distinct_sort = count_distinct.sortByKey().sortByKey(1)\n",
    "count_distinct_sort.saveAsTextFile(\"BDA/output/count_distinct_sort\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e27f433-8d96-44ad-a726-c429c8e92f93",
   "metadata": {},
   "source": [
    "* Output of countsort (the number of readings for each month)\n",
    "((u'1950', u'03'), 81)  \n",
    "((u'1950', u'04'), 352)  \n",
    "((u'1950', u'05'), 2802)  \n",
    "((u'1950', u'06'), 4886)  \n",
    "((u'1950', u'07'), 5811)  \n",
    "((u'1950', u'08'), 5954)  \n",
    "((u'1950', u'09'), 3612)  \n",
    "((u'1950', u'10'), 1248)  \n",
    "((u'1950', u'11'), 2)  \n",
    "((u'1950', u'12'), 1)  \n",
    "((u'1951', u'02'), 1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51cdc80-6e81-4e1d-b3ed-5389ea558022",
   "metadata": {},
   "source": [
    "* Output of count_distinct_sort(the number of distinct readings for each month)\n",
    "((u'1950', u'03'), 26)  \n",
    "((u'1950', u'04'), 36)  \n",
    "((u'1950', u'05'), 46)  \n",
    "((u'1950', u'06'), 47)  \n",
    "((u'1950', u'07'), 49)  \n",
    "((u'1950', u'08'), 49)  \n",
    "((u'1950', u'09'), 50)  \n",
    "((u'1950', u'10'), 46)  \n",
    "((u'1950', u'11'), 2)  \n",
    "((u'1950', u'12'), 1)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c578c933-6dc0-4f7c-af4c-e6a686347ee6",
   "metadata": {},
   "source": [
    "### Q4 Provide a list of stations with their associated maximum measured temperatures and\n",
    "maximum measured daily precipitation. Show only those stations where the maximum\n",
    "temperature is between 25 and 30 degrees and maximum daily precipitation is between 100\n",
    "mm and 200mm.\n",
    "In this exercise you will use the temperature-readings.csv and precipitation-readings.csv\n",
    "files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f756ec05-f4b9-419f-9cc0-59660338bb11",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[0;32m      3\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(appName \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexercise 4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This path is to the file on hdfs\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName = \"exercise 4\")\n",
    "# This path is to the file on hdfs\n",
    "temperature_file = sc.textFile(\"BDA/input/temperature-readings.csv\")\n",
    "precipitation_file = sc.textFile(\"BDA/input/precipitation-readings.csv\")\n",
    "\n",
    "temperature_lines = temperature_file.map(lambda line: line.split(\";\"))\n",
    "precipitation_file = precipitation_file.map(lambda line: line.split(\";\"))\n",
    "\n",
    "get_temperature = temperature_lines.map(lambda x: (x[0],float(x[3])))\n",
    "get_percipitation = precipitation_file.map(lambda x: (x[0],float(x[3])))\n",
    "\n",
    "max_temp = get_temperature.reduceByKey(max)\n",
    "filter_temp = max_temp.filter(lambda x : x[1]>25 and x[1]<30)\n",
    "\n",
    "\n",
    "max_perc = get_percipitation.reduceByKey(max)\n",
    "filter_perc = max_perc.filter(lambda x : x[1]>1000 and x[1]<200)\n",
    "\n",
    "\n",
    "\n",
    "join_output= filter_temp.join(filter_perc)\n",
    "join_output = join_output.coalesce(1)\n",
    "join_output_sort = join_output.sortByKey()\n",
    "join_output_sort.saveAsTextFile(\"BDA/output/temp_perce_sort\")\n",
    "\n",
    "### Below are for testing since no output\n",
    "#filter_perc = max_perc.filter(lambda x : x[1]>0 and x[1]<15) \n",
    "#filter_perc = max_perc.filter(lambda x : x[1]>100) \n",
    "#filter_temp.saveAsTextFile(\"BDA/output/temp_test\")\n",
    "#max_perc.saveAsTextFile(\"BDA/output/maxperc_test\")\n",
    "#filter_perc.saveAsTextFile(\"BDA/output/filterperc_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624ed81a-079f-4a66-ba0a-aa0b575480fc",
   "metadata": {},
   "source": [
    "* The output for temp_perce_sort is empty, since no data meet the criteria"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
